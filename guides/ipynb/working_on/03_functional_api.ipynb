{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# The Functional API\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2019/03/01<br>\n",
    "**Last modified:** 2020/04/12<br>\n",
    "**Description:** Complete guide to the functional API."
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-07 20:31:27.595661: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-07 20:31:27.595683: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "The Keras *functional API* is a way to create models that are more flexible\n",
    "than the `tf.keras.Sequential` API. The functional API can handle models\n",
    "with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "The main idea is that a deep learning model is usually\n",
    "a directed acyclic graph (DAG) of layers.\n",
    "So the functional API is a way to build *graphs of layers*.\n",
    "\n",
    "Consider the following model:\n",
    "\n",
    "<div class=\"k-default-codeblock\">\n",
    "```\n",
    "(input: 784-dimensional vectors)\n",
    "       ↧\n",
    "[Dense (64 units, relu activation)]\n",
    "       ↧\n",
    "[Dense (64 units, relu activation)]\n",
    "       ↧\n",
    "[Dense (10 units, softmax activation)]\n",
    "       ↧\n",
    "(output: logits of a probability distribution over 10 classes)\n",
    "```\n",
    "</div>\n",
    "\n",
    "This is a basic graph with three layers.\n",
    "To build this model using the functional API, start by creating an input node:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "inputs = keras.Input(shape=(784,))"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The shape of the data is set as a 784-dimensional vector.\n",
    "The batch size is always omitted since only the shape of each sample is specified.\n",
    "\n",
    "If, for example, you have an image input with a shape of `(32, 32, 3)`,\n",
    "you would use:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Just for demonstration purposes.\n",
    "img_inputs = keras.Input(shape=(32, 32, 3))"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `inputs` that is returned contains information about the shape and `dtype`\n",
    "of the input data that you feed to your model.\n",
    "Here's the shape:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "inputs.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([None, 784])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's the dtype:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputs.dtype"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You create a new node in the graph of layers by calling a layer on this `inputs`\n",
    "object:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "x = dense(inputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-07 20:31:29.215963: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-07 20:31:29.216043: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-07 20:31:29.216093: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nikoenki-XPS-13-7390-2-in-1): /proc/driver/nvidia/version does not exist\n",
      "2021-08-07 20:31:29.216670: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The \"layer call\" action is like drawing an arrow from \"inputs\" to this layer\n",
    "you created.\n",
    "You're \"passing\" the inputs to the `dense` layer, and you get `x` as the output.\n",
    "\n",
    "Let's add a few more layers to the graph of layers:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10)(x)"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point, you can create a `Model` by specifying its inputs and outputs\n",
    "in the graph of layers:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check out what the model summary looks like:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also plot the model as a graph:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keras.utils.plot_model(model, \"my_first_model.png\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And, optionally, display the input and output shapes of each layer\n",
    "in the plotted graph:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This figure and the code are almost identical. In the code version,\n",
    "the connection arrows are replaced by the call operation.\n",
    "\n",
    "A \"graph of layers\" is an intuitive mental image for a deep learning model,\n",
    "and the functional API is a way to create models that closely mirrors this."
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training, evaluation, and inference\n",
    "\n",
    "Training, evaluation, and inference work exactly in the same way for models\n",
    "built using the functional API as for `Sequential` models.\n",
    "\n",
    "The `Model` class offers a built-in training loop (the `fit()` method)\n",
    "and a built-in evaluation loop (the `evaluate()` method). Note\n",
    "that you can easily [customize these loops](/guides/customizing_what_happens_in_fit/)\n",
    "to implement training routines beyond supervised learning\n",
    "(e.g. [GANs](/examples/generative/dcgan_overriding_train_step/)).\n",
    "\n",
    "Here, load the MNIST image data, reshape it into vectors,\n",
    "fit the model on the data (while monitoring performance on a validation split),\n",
    "then evaluate the model on the test data:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_split=0.2)\n",
    "\n",
    "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-07 20:31:29.915923: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-08-07 20:31:29.917441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1497600000 Hz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3551 - accuracy: 0.9011 - val_loss: 0.1954 - val_accuracy: 0.9427\n",
      "Epoch 2/2\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.1614 - accuracy: 0.9536 - val_loss: 0.1392 - val_accuracy: 0.9605\n",
      "313/313 - 0s - loss: 0.1413 - accuracy: 0.9564\n",
      "Test loss: 0.14125922322273254\n",
      "Test accuracy: 0.9563999772071838\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For further reading, see the [training and evaluation](/guides/training_with_built_in_methods/) guide."
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save and serialize\n",
    "\n",
    "Saving the model and serialization work the same way for models built using\n",
    "the functional API as they do for `Sequential` models. The standard way\n",
    "to save a functional model is to call `model.save()`\n",
    "to save the entire model as a single file. You can later recreate the same model\n",
    "from this file, even if the code that built the model is no longer available.\n",
    "\n",
    "This saved file includes the:\n",
    "- model architecture\n",
    "- model weight values (that were learned during training)\n",
    "- model training config, if any (as passed to `compile`)\n",
    "- optimizer and its state, if any (to restart training where you left off)"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"path_to_my_model\")\n",
    "del model\n",
    "# Recreate the exact same model purely from the file:\n",
    "model = keras.models.load_model(\"path_to_my_model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-07 20:31:33.183052: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: path_to_my_model/assets\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For details, read the model [serialization & saving](\n",
    "    /guides/serialization_and_saving/) guide."
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use the same graph of layers to define multiple models\n",
    "\n",
    "In the functional API, models are created by specifying their inputs\n",
    "and outputs in a graph of layers. That means that a single\n",
    "graph of layers can be used to generate multiple models.\n",
    "\n",
    "In the example below, you use the same stack of layers to instantiate two models:\n",
    "an `encoder` model that turns image inputs into 16-dimensional vectors,\n",
    "and an end-to-end `autoencoder` model for training."
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "x = layers.Reshape((4, 4, 1))(encoder_output)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
    "\n",
    "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
    "autoencoder.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 6, 32)          9248      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 16)          4624      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 18,672\n",
      "Trainable params: 18,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 6, 32)          9248      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 16)          4624      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 6, 6, 16)          160       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 8, 8, 32)          4640      \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 26, 26, 16)        4624      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 28,241\n",
      "Trainable params: 28,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, the decoding architecture is strictly symmetrical\n",
    "to the encoding architecture, so the output shape is the same as\n",
    "the input shape `(28, 28, 1)`.\n",
    "\n",
    "The reverse of a `Conv2D` layer is a `Conv2DTranspose` layer,\n",
    "and the reverse of a `MaxPooling2D` layer is an `UpSampling2D` layer."
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All models are callable, just like layers\n",
    "\n",
    "You can treat any model as if it were a layer by invoking it on an `Input` or\n",
    "on the output of another layer. By calling a model you aren't just reusing\n",
    "the architecture of the model, you're also reusing its weights.\n",
    "\n",
    "To see this in action, here's a different take on the autoencoder example that\n",
    "creates an encoder model, a decoder model, and chains them in two calls\n",
    "to obtain the autoencoder model:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "decoder_input = keras.Input(shape=(16,), name=\"encoded_img\")\n",
    "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
    "\n",
    "decoder = keras.Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "autoencoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "encoded_img = encoder(autoencoder_input)\n",
    "decoded_img = decoder(encoded_img)\n",
    "autoencoder = keras.Model(autoencoder_input, decoded_img, name=\"autoencoder\")\n",
    "autoencoder.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "original_img (InputLayer)    [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 32)          9248      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 16)          4624      \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 18,672\n",
      "Trainable params: 18,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoded_img (InputLayer)     [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4, 4, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 6, 6, 16)          160       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 8, 8, 32)          4640      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 26, 26, 16)        4624      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 9,569\n",
      "Trainable params: 9,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         (None, 16)                18672     \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 28, 28, 1)         9569      \n",
      "=================================================================\n",
      "Total params: 28,241\n",
      "Trainable params: 28,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the model can be nested: a model can contain sub-models\n",
    "(since a model is just like a layer).\n",
    "A common use case for model nesting is *ensembling*.\n",
    "For example, here's how to ensemble a set of models into a single model\n",
    "that averages their predictions:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def get_model():\n",
    "    inputs = keras.Input(shape=(128,))\n",
    "    outputs = layers.Dense(1)(inputs)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model1 = get_model()\n",
    "model2 = get_model()\n",
    "model3 = get_model()\n",
    "\n",
    "inputs = keras.Input(shape=(128,))\n",
    "y1 = model1(inputs)\n",
    "y2 = model2(inputs)\n",
    "y3 = model3(inputs)\n",
    "outputs = layers.average([y1, y2, y3])\n",
    "ensemble_model = keras.Model(inputs=inputs, outputs=outputs)"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model1.summary()\n",
    "ensemble_model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 129\n",
      "Trainable params: 129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 1)            129         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, 1)            129         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Functional)            (None, 1)            129         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average (Average)               (None, 1)            0           model[0][0]                      \n",
      "                                                                 model_1[0][0]                    \n",
      "                                                                 model_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 387\n",
      "Trainable params: 387\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Manipulate complex graph topologies\n",
    "\n",
    "### Models with multiple inputs and outputs\n",
    "\n",
    "The functional API makes it easy to manipulate multiple inputs and outputs.\n",
    "This cannot be handled with the `Sequential` API.\n",
    "\n",
    "For example, if you're building a system for ranking customer issue tickets by\n",
    "priority and routing them to the correct department,\n",
    "then the model will have three inputs:\n",
    "\n",
    "- the title of the ticket (text input),\n",
    "- the text body of the ticket (text input), and\n",
    "- any tags added by the user (categorical input)\n",
    "\n",
    "This model will have two outputs:\n",
    "\n",
    "- the priority score between 0 and 1 (scalar sigmoid output), and\n",
    "- the department that should handle the ticket (softmax output\n",
    "over the set of departments).\n",
    "\n",
    "You can build this model in a few lines with the functional API:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_tags = 12  # Number of unique issue tags\n",
    "num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
    "num_departments = 4  # Number of departments for predictions\n",
    "\n",
    "title_input = keras.Input(\n",
    "    shape=(None,), name=\"title\"\n",
    ")  # Variable-length sequence of ints\n",
    "body_input = keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\n",
    "tags_input = keras.Input(\n",
    "    shape=(num_tags,), name=\"tags\"\n",
    ")  # Binary vectors of size `num_tags`\n",
    "\n",
    "# Embed each word in the title into a 64-dimensional vector\n",
    "title_features = layers.Embedding(num_words, 64)(title_input)\n",
    "# Embed each word in the text into a 64-dimensional vector\n",
    "body_features = layers.Embedding(num_words, 64)(body_input)\n",
    "\n",
    "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
    "title_features = layers.LSTM(128)(title_features)\n",
    "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
    "body_features = layers.LSTM(32)(body_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "x = layers.concatenate([title_features, body_features, tags_input])\n",
    "\n",
    "# Stick a logistic regression for priority prediction on top of the features\n",
    "priority_pred = layers.Dense(1, name=\"priority\")(x)\n",
    "# Stick a department classifier on top of the features\n",
    "department_pred = layers.Dense(num_departments, name=\"department\")(x)\n",
    "\n",
    "# Instantiate an end-to-end model predicting both priority and department\n",
    "model = keras.Model(\n",
    "    inputs=[title_input, body_input, tags_input],\n",
    "    outputs=[priority_pred, department_pred],\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now plot the model:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When compiling this model, you can assign different losses to each output.\n",
    "You can even assign different weights to each loss -- to modulate\n",
    "their contribution to the total training loss."
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[\n",
    "        keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    ],\n",
    "    loss_weights=[1.0, 0.2],\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the output layers have different names, you could also specify\n",
    "the loss like this:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"priority\": keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        \"department\": keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    },\n",
    "    loss_weights=[1.0, 0.2],\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model by passing lists of NumPy arrays of inputs and targets:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dummy input data\n",
    "title_data = np.random.randint(num_words, size=(1280, 10))\n",
    "body_data = np.random.randint(num_words, size=(1280, 100))\n",
    "tags_data = np.random.randint(2, size=(1280, num_tags)).astype(\"float32\")\n",
    "\n",
    "# Dummy target data\n",
    "priority_targets = np.random.random(size=(1280, 1))\n",
    "dept_targets = np.random.randint(2, size=(1280, num_departments))\n",
    "\n",
    "model.fit(\n",
    "    {\"title\": title_data, \"body\": body_data, \"tags\": tags_data},\n",
    "    {\"priority\": priority_targets, \"department\": dept_targets},\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "40/40 [==============================] - 3s 23ms/step - loss: 1.2857 - priority_loss: 0.7021 - department_loss: 2.9181\n",
      "Epoch 2/2\n",
      "40/40 [==============================] - 1s 23ms/step - loss: 1.2830 - priority_loss: 0.7016 - department_loss: 2.9073\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa355778e90>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When calling fit with a `Dataset` object, it should yield either a\n",
    "tuple of lists like `([title_data, body_data, tags_data], [priority_targets, dept_targets])`\n",
    "or a tuple of dictionaries like\n",
    "`({'title': title_data, 'body': body_data, 'tags': tags_data}, {'priority': priority_targets, 'department': dept_targets})`.\n",
    "\n",
    "For more detailed explanation, refer to the [training and evaluation](/guides/training_with_built_in_methods/) guide."
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A toy ResNet model\n",
    "\n",
    "In addition to models with multiple inputs and outputs,\n",
    "the functional API makes it easy to manipulate non-linear connectivity\n",
    "topologies -- these are models with layers that are not connected sequentially,\n",
    "which the `Sequential` API cannot handle.\n",
    "\n",
    "A common use case for this is residual connections.\n",
    "Let's build a toy ResNet model for CIFAR10 to demonstrate this:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3), name=\"img\")\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "block_1_output = layers.MaxPooling2D(3)(x)\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_1_output)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "block_2_output = layers.add([x, block_1_output])\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_2_output)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "block_3_output = layers.add([x, block_2_output])\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\")(block_3_output)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(10)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs, name=\"toy_resnet\")\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"toy_resnet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img (InputLayer)                [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 30, 30, 32)   896         img[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 28, 28, 64)   18496       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 9, 9, 64)     0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 9, 9, 64)     36928       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 9, 9, 64)     36928       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 9, 9, 64)     0           conv2d_11[0][0]                  \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 9, 9, 64)     36928       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 9, 9, 64)     36928       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 9, 9, 64)     0           conv2d_13[0][0]                  \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 7, 7, 64)     36928       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 64)           0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          16640       global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           2570        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 223,242\n",
      "Trainable params: 223,242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the model:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keras.utils.plot_model(model, \"mini_resnet.png\", show_shapes=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now train the model:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "# We restrict the data to the first 1000 samples so as to limit execution time\n",
    "# on Colab. Try to train on the entire dataset until convergence!\n",
    "model.fit(x_train[:1000], y_train[:1000], batch_size=64, epochs=1, validation_split=0.2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13/13 [==============================] - 2s 95ms/step - loss: 2.3710 - acc: 0.1175 - val_loss: 2.2994 - val_acc: 0.0900\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa349b56f10>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shared layers\n",
    "\n",
    "Another good use for the functional API are models that use *shared layers*.\n",
    "Shared layers are layer instances that are reused multiple times in the same model --\n",
    "they learn features that correspond to multiple paths in the graph-of-layers.\n",
    "\n",
    "Shared layers are often used to encode inputs from similar spaces\n",
    "(say, two different pieces of text that feature similar vocabulary).\n",
    "They enable sharing of information across these different inputs,\n",
    "and they make it possible to train such a model on less data.\n",
    "If a given word is seen in one of the inputs,\n",
    "that will benefit the processing of all inputs that pass through the shared layer.\n",
    "\n",
    "To share a layer in the functional API, call the same layer instance multiple times.\n",
    "For instance, here's an `Embedding` layer shared across two different text inputs:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Embedding for 1000 unique words mapped to 128-dimensional vectors\n",
    "shared_embedding = layers.Embedding(1000, 128)\n",
    "\n",
    "# Variable-length sequence of integers\n",
    "text_input_a = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Variable-length sequence of integers\n",
    "text_input_b = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "# Reuse the same layer to encode both inputs\n",
    "encoded_input_a = shared_embedding(text_input_a)\n",
    "encoded_input_b = shared_embedding(text_input_b)"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract and reuse nodes in the graph of layers\n",
    "\n",
    "Because the graph of layers you are manipulating is a static data structure,\n",
    "it can be accessed and inspected. And this is how you are able to plot\n",
    "functional models as images.\n",
    "\n",
    "This also means that you can access the activations of intermediate layers\n",
    "(\"nodes\" in the graph) and reuse them elsewhere --\n",
    "which is very useful for something like feature extraction.\n",
    "\n",
    "Let's look at an example. This is a VGG19 model with weights pretrained on ImageNet:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "vgg19 = tf.keras.applications.VGG19()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
      "574717952/574710816 [==============================] - 251s 0us/step\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And these are the intermediate activations of the model,\n",
    "obtained by querying the graph data structure:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "features_list = [layer.output for layer in vgg19.layers]"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use these features to create a new feature-extraction model that returns\n",
    "the values of the intermediate layer activations:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)\n",
    "\n",
    "img = np.random.random((1, 224, 224, 3)).astype(\"float32\")\n",
    "extracted_features = feat_extraction_model(img)"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "extracted_features"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 224, 224, 3), dtype=float32, numpy=\n",
       " array([[[[0.2592841 , 0.61335635, 0.10212464],\n",
       "          [0.7891346 , 0.0978823 , 0.33769527],\n",
       "          [0.93936855, 0.66268235, 0.10388305],\n",
       "          ...,\n",
       "          [0.78885186, 0.6208915 , 0.01539324],\n",
       "          [0.02755762, 0.40624845, 0.78247106],\n",
       "          [0.89138484, 0.6590479 , 0.75574285]],\n",
       " \n",
       "         [[0.29056224, 0.20432208, 0.38644692],\n",
       "          [0.80759764, 0.69189066, 0.8181804 ],\n",
       "          [0.03169566, 0.9434614 , 0.49688825],\n",
       "          ...,\n",
       "          [0.7897075 , 0.86143386, 0.97217655],\n",
       "          [0.7871466 , 0.39014798, 0.9474003 ],\n",
       "          [0.12598424, 0.67097646, 0.44947535]],\n",
       " \n",
       "         [[0.9584447 , 0.95511675, 0.76380205],\n",
       "          [0.16596532, 0.89707285, 0.08841325],\n",
       "          [0.5004119 , 0.30295002, 0.02324103],\n",
       "          ...,\n",
       "          [0.26996478, 0.21663383, 0.5113034 ],\n",
       "          [0.4375979 , 0.50942975, 0.19144338],\n",
       "          [0.6571216 , 0.6480191 , 0.83614844]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.2558047 , 0.88034767, 0.15862297],\n",
       "          [0.7788471 , 0.43700463, 0.19868362],\n",
       "          [0.5890421 , 0.7399696 , 0.38658723],\n",
       "          ...,\n",
       "          [0.19453461, 0.1436042 , 0.51657873],\n",
       "          [0.18889184, 0.73592496, 0.2647934 ],\n",
       "          [0.19193394, 0.978193  , 0.4138638 ]],\n",
       " \n",
       "         [[0.25594497, 0.51834106, 0.32032335],\n",
       "          [0.18818541, 0.8837771 , 0.8065981 ],\n",
       "          [0.55377924, 0.72293794, 0.8919678 ],\n",
       "          ...,\n",
       "          [0.74792045, 0.6225712 , 0.5909287 ],\n",
       "          [0.8485792 , 0.24242818, 0.2552029 ],\n",
       "          [0.44667146, 0.25663465, 0.33912352]],\n",
       " \n",
       "         [[0.02804053, 0.47621417, 0.36901402],\n",
       "          [0.5616033 , 0.15549047, 0.45799008],\n",
       "          [0.84494334, 0.03202117, 0.20544498],\n",
       "          ...,\n",
       "          [0.9450254 , 0.68088984, 0.36867884],\n",
       "          [0.14127748, 0.08201249, 0.40379322],\n",
       "          [0.23690268, 0.66318446, 0.57556814]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 224, 224, 64), dtype=float32, numpy=\n",
       " array([[[[0.        , 0.15935297, 0.        , ..., 0.55374783,\n",
       "           0.31101292, 0.        ],\n",
       "          [0.        , 0.36102128, 0.31059712, ..., 0.30244088,\n",
       "           1.1081021 , 0.6361767 ],\n",
       "          [0.        , 0.43992838, 0.40435866, ..., 0.14591151,\n",
       "           1.4298903 , 0.9863515 ],\n",
       "          ...,\n",
       "          [0.        , 0.22870916, 0.3295892 , ..., 0.12653625,\n",
       "           1.564075  , 1.1175725 ],\n",
       "          [0.        , 0.21967438, 0.3096087 , ..., 0.04251206,\n",
       "           1.43063   , 0.93905884],\n",
       "          [0.76919323, 0.252412  , 0.59399205, ..., 0.577571  ,\n",
       "           2.113089  , 2.0806556 ]],\n",
       " \n",
       "         [[0.        , 0.2675873 , 0.03896291, ..., 0.18882906,\n",
       "           0.        , 0.        ],\n",
       "          [0.5656434 , 0.51276135, 0.5438889 , ..., 0.        ,\n",
       "           0.7831576 , 0.8160702 ],\n",
       "          [1.0556625 , 0.5380897 , 0.53358096, ..., 0.        ,\n",
       "           0.49569672, 0.7317679 ],\n",
       "          ...,\n",
       "          [0.955741  , 0.257791  , 0.34977287, ..., 0.        ,\n",
       "           0.5278998 , 0.87905896],\n",
       "          [0.8665581 , 0.25066757, 0.37540033, ..., 0.        ,\n",
       "           0.5622891 , 0.85897297],\n",
       "          [2.0980825 , 0.33773333, 0.6471702 , ..., 0.17789346,\n",
       "           1.6082287 , 1.9202168 ]],\n",
       " \n",
       "         [[0.        , 0.3095641 , 0.20769724, ..., 0.21372116,\n",
       "           0.02968219, 0.11345214],\n",
       "          [0.8668599 , 0.3909071 , 0.46882215, ..., 0.        ,\n",
       "           0.64029604, 0.6828708 ],\n",
       "          [0.70643634, 0.3738127 , 0.36874685, ..., 0.        ,\n",
       "           0.35799265, 0.27128977],\n",
       "          ...,\n",
       "          [1.295903  , 0.26577735, 0.21225794, ..., 0.        ,\n",
       "           0.18589929, 0.45735058],\n",
       "          [0.8947042 , 0.15511325, 0.08836932, ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [1.8487768 , 0.442041  , 0.6801401 , ..., 0.291633  ,\n",
       "           1.626229  , 1.8970344 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.1840847 , 0.10788406, ..., 0.30313873,\n",
       "           0.        , 0.        ],\n",
       "          [0.20049858, 0.31413597, 0.44073468, ..., 0.        ,\n",
       "           0.5643452 , 0.44966912],\n",
       "          [0.30004808, 0.21139687, 0.4339721 , ..., 0.        ,\n",
       "           0.85497475, 0.78032297],\n",
       "          ...,\n",
       "          [1.5277538 , 0.3469223 , 0.46574658, ..., 0.        ,\n",
       "           0.7856026 , 0.974327  ],\n",
       "          [0.7717863 , 0.1915023 , 0.2502744 , ..., 0.        ,\n",
       "           0.4064049 , 0.420262  ],\n",
       "          [1.6826656 , 0.33974388, 0.5985552 , ..., 0.4764868 ,\n",
       "           1.4149317 , 1.666023  ]],\n",
       " \n",
       "         [[0.        , 0.12156575, 0.04473753, ..., 0.34261662,\n",
       "           0.        , 0.        ],\n",
       "          [0.7152633 , 0.30683053, 0.3576253 , ..., 0.        ,\n",
       "           0.16234463, 0.4376195 ],\n",
       "          [1.2124002 , 0.35642463, 0.45157617, ..., 0.        ,\n",
       "           0.6382317 , 0.9909025 ],\n",
       "          ...,\n",
       "          [1.119863  , 0.4751716 , 0.54437304, ..., 0.        ,\n",
       "           1.0622556 , 1.1814845 ],\n",
       "          [0.81598735, 0.32019857, 0.35959047, ..., 0.        ,\n",
       "           0.62176996, 0.7037953 ],\n",
       "          [1.8378501 , 0.33953938, 0.55745596, ..., 0.46838552,\n",
       "           1.1982685 , 1.4408143 ]],\n",
       " \n",
       "         [[0.7711117 , 0.02091113, 0.        , ..., 0.611012  ,\n",
       "           0.        , 0.        ],\n",
       "          [1.8461101 , 0.14606136, 0.17425679, ..., 0.30098152,\n",
       "           0.        , 0.31214222],\n",
       "          [2.1828883 , 0.265886  , 0.28442693, ..., 0.21240157,\n",
       "           0.        , 0.5159788 ],\n",
       "          ...,\n",
       "          [2.437484  , 0.4794843 , 0.582011  , ..., 0.2924255 ,\n",
       "           0.54365045, 1.3040023 ],\n",
       "          [2.0170772 , 0.2825771 , 0.32494366, ..., 0.25156087,\n",
       "           0.23328538, 0.7895644 ],\n",
       "          [1.933294  , 0.23719797, 0.38002658, ..., 0.7368178 ,\n",
       "           0.8103639 , 1.2311896 ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 224, 224, 64), dtype=float32, numpy=\n",
       " array([[[[5.4663134 , 1.0040685 , 1.4108522 , ..., 0.5755278 ,\n",
       "           2.83273   , 1.86164   ],\n",
       "          [1.6880207 , 1.328464  , 2.8154619 , ..., 1.0861644 ,\n",
       "           1.8963876 , 1.8790147 ],\n",
       "          [0.12358671, 0.73717624, 2.6044648 , ..., 1.0007232 ,\n",
       "           0.606353  , 1.4391758 ],\n",
       "          ...,\n",
       "          [1.0556369 , 0.3952546 , 1.8561333 , ..., 1.8813864 ,\n",
       "           1.1622784 , 1.5290952 ],\n",
       "          [0.        , 0.        , 1.6133982 , ..., 1.6184416 ,\n",
       "           1.0362383 , 1.7163296 ],\n",
       "          [0.        , 0.        , 0.7532391 , ..., 1.1663041 ,\n",
       "           0.        , 0.59561986]],\n",
       " \n",
       "         [[2.3780468 , 0.84922713, 2.4521353 , ..., 1.1863735 ,\n",
       "           2.8572545 , 0.3582396 ],\n",
       "          [0.        , 0.        , 4.5954037 , ..., 1.3416224 ,\n",
       "           2.6994948 , 0.        ],\n",
       "          [0.        , 0.        , 3.9965448 , ..., 1.1355793 ,\n",
       "           1.8407373 , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 3.9516366 , ..., 2.5444736 ,\n",
       "           5.676958  , 0.        ],\n",
       "          [0.        , 0.        , 3.5332253 , ..., 1.9855928 ,\n",
       "           6.866267  , 0.07219642],\n",
       "          [0.        , 0.        , 1.694528  , ..., 1.6314489 ,\n",
       "           2.3994088 , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.2129308 , 1.9499251 , ..., 0.5153973 ,\n",
       "           2.715566  , 0.35014707],\n",
       "          [0.        , 0.        , 3.5047667 , ..., 0.5543274 ,\n",
       "           1.9249586 , 0.5866222 ],\n",
       "          [3.475706  , 1.4727042 , 2.6718264 , ..., 1.1175985 ,\n",
       "           0.        , 0.8016587 ],\n",
       "          ...,\n",
       "          [0.09688252, 1.2004545 , 4.0170603 , ..., 2.043466  ,\n",
       "           0.        , 1.2111065 ],\n",
       "          [2.441705  , 0.        , 3.7827437 , ..., 1.7604882 ,\n",
       "           4.2542744 , 1.1815348 ],\n",
       "          [0.5808762 , 0.17313689, 2.1550827 , ..., 1.7480452 ,\n",
       "           3.1793265 , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[2.2804744 , 1.1477656 , 1.0793719 , ..., 0.5933156 ,\n",
       "           1.9443771 , 0.65622014],\n",
       "          [1.0192869 , 1.6819515 , 2.4390378 , ..., 1.5214219 ,\n",
       "           4.074301  , 0.69318646],\n",
       "          [0.70526475, 1.6787987 , 2.106738  , ..., 2.1432645 ,\n",
       "           0.        , 0.65101856],\n",
       "          ...,\n",
       "          [0.22823268, 0.        , 3.579057  , ..., 1.8104664 ,\n",
       "           1.420356  , 0.7456854 ],\n",
       "          [2.2044494 , 0.        , 2.574292  , ..., 1.8764585 ,\n",
       "           0.9462524 , 0.877435  ],\n",
       "          [0.        , 0.3780673 , 1.2095305 , ..., 1.6000553 ,\n",
       "           1.2197627 , 0.        ]],\n",
       " \n",
       "         [[0.20541912, 0.        , 1.2296212 , ..., 0.9457094 ,\n",
       "           1.694143  , 1.0413115 ],\n",
       "          [0.        , 0.        , 2.7635915 , ..., 1.3700883 ,\n",
       "           5.889737  , 1.6679113 ],\n",
       "          [0.        , 0.        , 2.7280788 , ..., 1.2632651 ,\n",
       "           4.0942936 , 2.024282  ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 4.2869196 , ..., 1.7237289 ,\n",
       "           2.0459712 , 1.9915204 ],\n",
       "          [0.        , 0.        , 3.7673628 , ..., 1.5683632 ,\n",
       "           1.4239674 , 1.6111476 ],\n",
       "          [0.        , 0.        , 2.0213695 , ..., 1.4120904 ,\n",
       "           1.8293697 , 0.25059742]],\n",
       " \n",
       "         [[0.        , 0.        , 0.89293957, ..., 0.58628035,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 2.0219204 , ..., 0.7788706 ,\n",
       "           1.7229208 , 0.        ],\n",
       "          [0.7232618 , 0.        , 2.2731233 , ..., 0.92335033,\n",
       "           2.7752445 , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 2.9335024 , ..., 0.6060758 ,\n",
       "           3.8004894 , 0.        ],\n",
       "          [1.1554081 , 0.        , 2.6840563 , ..., 0.8665338 ,\n",
       "           2.2482789 , 0.        ],\n",
       "          [2.4873428 , 1.0481355 , 1.4742084 , ..., 1.190492  ,\n",
       "           3.1173787 , 0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 112, 112, 64), dtype=float32, numpy=\n",
       " array([[[[5.4663134e+00, 1.3284640e+00, 4.5954037e+00, ...,\n",
       "           1.3416224e+00, 2.8572545e+00, 1.8790147e+00],\n",
       "          [1.4620085e+00, 7.3717624e-01, 3.9965448e+00, ...,\n",
       "           1.7422922e+00, 4.4242167e+00, 1.4391758e+00],\n",
       "          [6.3988060e-01, 4.1402835e-01, 1.9787303e+00, ...,\n",
       "           2.1362908e+00, 4.6890965e+00, 1.2489355e+00],\n",
       "          ...,\n",
       "          [1.9045434e+00, 6.3813829e-01, 5.7473631e+00, ...,\n",
       "           2.2511115e+00, 3.7597067e+00, 1.5384049e+00],\n",
       "          [1.0556369e+00, 3.9525461e-01, 4.5993342e+00, ...,\n",
       "           2.5444736e+00, 5.6769581e+00, 1.5290952e+00],\n",
       "          [0.0000000e+00, 0.0000000e+00, 3.5332253e+00, ...,\n",
       "           1.9855928e+00, 6.8662672e+00, 1.7163296e+00]],\n",
       " \n",
       "         [[4.3853849e-01, 3.6902237e-01, 3.5047667e+00, ...,\n",
       "           6.9025099e-01, 4.0230641e+00, 5.8662218e-01],\n",
       "          [3.4757061e+00, 1.4807465e+00, 2.6718264e+00, ...,\n",
       "           1.6487972e+00, 1.7208186e+00, 8.0165869e-01],\n",
       "          [4.8625116e+00, 1.3154116e+00, 1.9013301e+00, ...,\n",
       "           2.7925818e+00, 1.3717448e+00, 1.0822096e+00],\n",
       "          ...,\n",
       "          [3.5896091e+00, 1.8814077e+00, 5.4351225e+00, ...,\n",
       "           2.1030250e+00, 1.1911922e+00, 8.0582982e-01],\n",
       "          [2.2804947e+00, 2.1787882e+00, 4.8912945e+00, ...,\n",
       "           2.2629430e+00, 2.9711971e+00, 1.2111065e+00],\n",
       "          [2.4417050e+00, 5.5545002e-01, 4.7051778e+00, ...,\n",
       "           2.0746505e+00, 4.2542744e+00, 1.1815348e+00]],\n",
       " \n",
       "         [[1.3382382e+00, 6.8326789e-01, 3.0930374e+00, ...,\n",
       "           8.1657034e-01, 2.5719295e+00, 7.6912481e-01],\n",
       "          [1.8108611e+00, 3.2593709e-01, 4.4607949e+00, ...,\n",
       "           1.9069755e+00, 3.3659813e+00, 8.2549459e-01],\n",
       "          [3.4786029e+00, 1.8620872e+00, 4.5827990e+00, ...,\n",
       "           3.0150127e+00, 4.4510159e+00, 1.0210238e+00],\n",
       "          ...,\n",
       "          [2.4866447e+00, 1.4103873e+00, 3.3777089e+00, ...,\n",
       "           2.3988719e+00, 8.5123563e-01, 6.8401402e-01],\n",
       "          [0.0000000e+00, 0.0000000e+00, 4.3731232e+00, ...,\n",
       "           1.2509700e+00, 3.8976107e+00, 9.0675062e-01],\n",
       "          [1.6316023e+00, 1.9488984e-01, 5.2525520e+00, ...,\n",
       "           1.6345947e+00, 4.2725840e+00, 9.2377621e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1.5870469e+00, 6.3517612e-01, 4.9585648e+00, ...,\n",
       "           1.2776911e+00, 5.3235569e+00, 4.4936234e-01],\n",
       "          [1.4263482e+00, 1.2388623e+00, 7.3278418e+00, ...,\n",
       "           2.7466259e+00, 5.5425820e+00, 3.5371941e-01],\n",
       "          [7.3003525e-01, 2.0424807e+00, 7.6029110e+00, ...,\n",
       "           2.8957210e+00, 3.6830904e+00, 1.4259571e-01],\n",
       "          ...,\n",
       "          [2.6621261e+00, 1.5621963e+00, 3.9363239e+00, ...,\n",
       "           2.7216666e+00, 3.6146681e+00, 9.9293321e-01],\n",
       "          [3.0993426e+00, 1.3531294e+00, 4.8140707e+00, ...,\n",
       "           3.0061743e+00, 3.4825790e+00, 8.9801437e-01],\n",
       "          [1.2052643e-01, 2.7449727e-03, 4.0187426e+00, ...,\n",
       "           2.6186271e+00, 3.1268005e+00, 7.4091715e-01]],\n",
       " \n",
       "         [[2.2804744e+00, 1.6819515e+00, 2.4390378e+00, ...,\n",
       "           1.5214219e+00, 4.0743008e+00, 9.2345041e-01],\n",
       "          [3.7342601e+00, 2.2743659e+00, 2.1067381e+00, ...,\n",
       "           2.2221072e+00, 1.3448604e+00, 9.9364287e-01],\n",
       "          [1.4006603e+00, 8.8587493e-01, 3.7710335e+00, ...,\n",
       "           1.8738140e+00, 3.5137753e+00, 8.4461838e-01],\n",
       "          ...,\n",
       "          [4.7120728e+00, 2.8356025e+00, 4.7345562e+00, ...,\n",
       "           1.9780031e+00, 2.7036753e+00, 1.0403218e+00],\n",
       "          [2.2823268e-01, 5.9724945e-01, 5.1026525e+00, ...,\n",
       "           2.2273428e+00, 3.7730012e+00, 7.4568540e-01],\n",
       "          [2.2044494e+00, 7.1903485e-01, 2.6882784e+00, ...,\n",
       "           2.2224898e+00, 1.3871801e+00, 8.7743503e-01]],\n",
       " \n",
       "         [[2.0541912e-01, 0.0000000e+00, 2.7635915e+00, ...,\n",
       "           1.3700883e+00, 5.8897371e+00, 1.6679113e+00],\n",
       "          [2.8383722e+00, 1.1722262e+00, 2.7280788e+00, ...,\n",
       "           1.4600639e+00, 4.0942936e+00, 2.0242820e+00],\n",
       "          [1.0722072e+00, 1.2509639e+00, 1.8808659e+00, ...,\n",
       "           1.7980726e+00, 2.5852220e+00, 1.5409126e+00],\n",
       "          ...,\n",
       "          [0.0000000e+00, 3.5455543e-01, 4.8575001e+00, ...,\n",
       "           2.1352642e+00, 3.1904557e+00, 2.1111174e+00],\n",
       "          [0.0000000e+00, 0.0000000e+00, 4.4723082e+00, ...,\n",
       "           1.7237289e+00, 3.8004894e+00, 2.1947529e+00],\n",
       "          [2.4873428e+00, 1.0481355e+00, 3.7673628e+00, ...,\n",
       "           1.5683632e+00, 3.1173787e+00, 1.6111476e+00]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 112, 112, 128), dtype=float32, numpy=\n",
       " array([[[[ 0.        ,  0.        ,  0.        , ...,  6.139717  ,\n",
       "            0.        , 13.439461  ],\n",
       "          [ 0.        ,  0.        ,  1.5285072 , ...,  4.313674  ,\n",
       "            0.        ,  1.2717229 ],\n",
       "          [ 0.        ,  0.        ,  3.6861768 , ..., 11.217095  ,\n",
       "            0.        ,  0.9795062 ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  5.0259314 ,\n",
       "            0.        ,  2.6759002 ],\n",
       "          [ 0.        ,  0.        ,  1.0271959 , ...,  6.0524893 ,\n",
       "            0.        ,  2.6680686 ],\n",
       "          [ 0.        ,  0.        ,  7.067737  , ..., 12.901519  ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  4.0909715 ,  0.        , ...,  6.2935753 ,\n",
       "            0.        , 15.951776  ],\n",
       "          [ 0.        ,  3.7076788 ,  1.3874968 , ...,  2.6082337 ,\n",
       "            0.05329688,  1.9306537 ],\n",
       "          [ 0.        ,  3.069122  ,  3.3289404 , ...,  5.0282564 ,\n",
       "            0.        ,  2.2547348 ],\n",
       "          ...,\n",
       "          [ 0.        ,  1.6847637 ,  0.        , ...,  2.0003114 ,\n",
       "            0.        ,  0.68068904],\n",
       "          [ 0.        ,  3.154594  ,  0.        , ...,  4.1208744 ,\n",
       "            0.        ,  1.4891287 ],\n",
       "          [ 0.        ,  2.507429  ,  7.1303177 , ..., 12.064219  ,\n",
       "            4.1424522 ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  2.5471668 ,  0.        , ...,  7.2218275 ,\n",
       "            0.        , 14.141625  ],\n",
       "          [ 0.        ,  2.3289049 ,  1.6045432 , ...,  5.4694457 ,\n",
       "            0.        ,  1.6131593 ],\n",
       "          [ 0.        ,  0.07070325,  1.7176216 , ...,  2.667933  ,\n",
       "            0.        ,  2.2221732 ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.73284864,  0.        , ...,  1.9199874 ,\n",
       "            1.0444087 ,  0.        ],\n",
       "          [ 0.        ,  2.2497454 ,  0.        , ...,  9.673814  ,\n",
       "            2.552897  ,  1.4867171 ],\n",
       "          [ 0.        ,  1.2561567 ,  7.4498024 , ..., 11.714295  ,\n",
       "            3.4640172 ,  0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.        ,  3.247081  ,  0.        , ...,  5.77262   ,\n",
       "            0.        , 18.315283  ],\n",
       "          [ 0.        ,  2.2559912 ,  1.7821102 , ...,  6.2530956 ,\n",
       "            0.        ,  1.2426797 ],\n",
       "          [ 0.        ,  0.        ,  1.9672763 , ...,  8.905087  ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.54056084, ...,  7.141564  ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.17431042,  0.        , ...,  6.8326235 ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  1.0122313 ,  5.5068893 , ..., 11.037183  ,\n",
       "            4.796244  ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  1.7246389 ,  0.        , ...,  4.8772864 ,\n",
       "            0.        , 16.774956  ],\n",
       "          [ 0.        ,  2.9253817 ,  0.        , ...,  2.1945407 ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  3.0257165 ,  0.48750982, ...,  9.061123  ,\n",
       "            1.082028  ,  0.04717605],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  3.126083  ,\n",
       "            0.        ,  0.19075696],\n",
       "          [ 0.        ,  0.65972614,  0.68299896, ...,  9.309663  ,\n",
       "            1.3335292 ,  0.        ],\n",
       "          [ 0.        ,  0.49229118,  7.5999136 , ...,  7.481797  ,\n",
       "            3.0379858 ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  3.8685849 ,  0.        , ...,  8.669002  ,\n",
       "            1.3742347 , 12.67122   ],\n",
       "          [ 0.        ,  4.0793476 ,  2.1681743 , ...,  6.8393755 ,\n",
       "            1.1423875 ,  0.        ],\n",
       "          [ 0.        ,  4.1990666 ,  0.9010769 , ...,  6.8054967 ,\n",
       "            0.25807267,  1.9051186 ],\n",
       "          ...,\n",
       "          [ 0.        ,  3.9987948 ,  0.        , ...,  7.350545  ,\n",
       "            2.8208914 ,  1.2060195 ],\n",
       "          [ 0.        ,  4.2768984 ,  2.6418254 , ...,  9.358247  ,\n",
       "            1.3244172 ,  0.38041037],\n",
       "          [ 0.        ,  2.4144793 ,  7.3149357 , ...,  8.010184  ,\n",
       "            0.6535604 ,  0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 112, 112, 128), dtype=float32, numpy=\n",
       " array([[[[ 0.        ,  6.440864  ,  2.4313908 , ...,  1.169494  ,\n",
       "            0.34913826,  0.        ],\n",
       "          [ 0.        ,  4.203434  ,  0.7743285 , ...,  7.3354917 ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 2.6453972 ,  3.7543256 ,  0.        , ...,  7.248243  ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 8.686739  ,  0.        ,  0.        , ...,  7.830946  ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 8.259502  ,  3.516772  ,  0.        , ..., 10.53474   ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 8.193455  ,  0.        ,  0.12897702, ..., 10.5360775 ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  7.8194623 ,  3.9847598 , ...,  3.8773174 ,\n",
       "           16.100498  ,  0.        ],\n",
       "          [ 0.        ,  0.38340753,  0.        , ..., 15.43456   ,\n",
       "           10.950516  ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ..., 14.917852  ,\n",
       "            4.5111594 ,  0.        ],\n",
       "          ...,\n",
       "          [ 2.6365712 ,  0.        ,  0.        , ..., 15.210415  ,\n",
       "            7.5117764 ,  0.        ],\n",
       "          [ 0.7264717 ,  0.        ,  0.        , ..., 19.703064  ,\n",
       "            0.64217705,  0.        ],\n",
       "          [ 4.8364897 ,  0.        ,  0.        , ..., 18.108028  ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        , 11.356751  ,  3.3983483 , ...,  2.8697298 ,\n",
       "           16.409878  ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ..., 13.472635  ,\n",
       "           12.828998  ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ..., 14.356082  ,\n",
       "            6.749973  ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  1.9629353 , ..., 10.436419  ,\n",
       "            7.557353  ,  0.        ],\n",
       "          [ 0.        ,  0.04155845,  0.        , ..., 11.236046  ,\n",
       "            2.1931791 ,  0.        ],\n",
       "          [ 0.8011591 ,  0.        ,  0.        , ..., 11.684912  ,\n",
       "            0.        ,  0.8317077 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.        ,  5.5448203 ,  0.        , ...,  4.508093  ,\n",
       "           16.553951  ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ..., 18.069828  ,\n",
       "           10.764965  ,  0.        ],\n",
       "          [ 0.        ,  2.038674  ,  0.        , ..., 17.148077  ,\n",
       "            1.3909376 ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ..., 17.930838  ,\n",
       "            3.6499996 ,  0.        ],\n",
       "          [ 3.7429004 ,  0.        ,  4.6483808 , ..., 17.191566  ,\n",
       "            0.        ,  0.        ],\n",
       "          [10.012387  ,  0.        ,  0.        , ..., 12.597319  ,\n",
       "            0.        ,  3.0588794 ]],\n",
       " \n",
       "         [[ 0.        ,  2.9368567 ,  0.        , ...,  5.8326    ,\n",
       "           21.9378    ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ..., 19.772493  ,\n",
       "           15.339387  ,  0.        ],\n",
       "          [ 0.        ,  5.6031585 ,  0.        , ..., 15.693292  ,\n",
       "            7.249369  ,  0.4310508 ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  4.6540093 , ..., 17.157963  ,\n",
       "            7.5116653 ,  0.        ],\n",
       "          [ 0.        ,  0.76886195,  2.2314506 , ..., 17.71426   ,\n",
       "            0.6626049 ,  0.        ],\n",
       "          [ 3.049763  ,  0.        ,  0.        , ..., 12.977569  ,\n",
       "            0.        ,  5.8987303 ]],\n",
       " \n",
       "         [[ 0.        ,  0.62010735,  0.85413384, ...,  1.2934417 ,\n",
       "           19.401903  ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  7.164746  ,\n",
       "           14.470756  ,  0.        ],\n",
       "          [ 0.        ,  1.099114  ,  0.        , ...,  3.5997195 ,\n",
       "           10.828394  ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  2.5831332 , ...,  5.0526986 ,\n",
       "           10.839549  ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  6.480856  ,\n",
       "            1.9825779 ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  5.898825  ,\n",
       "            0.        ,  4.640347  ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 56, 56, 128), dtype=float32, numpy=\n",
       " array([[[[ 0.        ,  7.8194623 ,  3.9847598 , ..., 15.43456   ,\n",
       "           16.100498  ,  0.        ],\n",
       "          [ 8.237447  ,  3.7543256 ,  0.        , ..., 14.917852  ,\n",
       "            4.5111594 ,  0.        ],\n",
       "          [ 9.424814  ,  2.3036342 ,  0.05684976, ..., 19.074194  ,\n",
       "            4.4803143 ,  0.        ],\n",
       "          ...,\n",
       "          [10.74342   ,  1.2552489 ,  3.7106483 , ..., 19.383608  ,\n",
       "            3.292024  ,  0.        ],\n",
       "          [10.907937  ,  0.        ,  1.5343076 , ..., 15.210415  ,\n",
       "            7.5117764 ,  0.        ],\n",
       "          [ 8.259502  ,  3.516772  ,  0.12897702, ..., 19.703064  ,\n",
       "            0.64217705,  0.        ]],\n",
       " \n",
       "         [[ 0.        , 11.356751  ,  3.3983483 , ..., 13.472635  ,\n",
       "           16.409878  ,  0.        ],\n",
       "          [ 3.1961975 ,  0.        ,  2.0236115 , ..., 17.023487  ,\n",
       "            7.752304  ,  0.        ],\n",
       "          [ 2.4121368 ,  0.        ,  1.089991  , ..., 18.283794  ,\n",
       "            4.751998  ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  7.547679  , ..., 14.3643    ,\n",
       "            9.624623  ,  0.        ],\n",
       "          [ 1.1013789 ,  0.        ,  3.8119538 , ..., 12.120595  ,\n",
       "            7.557353  ,  0.        ],\n",
       "          [ 3.5031354 ,  0.04155845,  1.2839196 , ..., 12.196678  ,\n",
       "            2.1931791 ,  0.8317077 ]],\n",
       " \n",
       "         [[ 3.1486256 ,  7.965195  ,  0.        , ..., 12.613788  ,\n",
       "           14.310287  ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ..., 13.209501  ,\n",
       "           13.651713  ,  0.        ],\n",
       "          [ 0.        ,  0.4434952 ,  0.        , ..., 14.790097  ,\n",
       "            6.4677896 ,  0.        ],\n",
       "          ...,\n",
       "          [ 2.9104817 ,  0.        ,  1.3469237 , ..., 13.5229845 ,\n",
       "            6.6820884 ,  0.        ],\n",
       "          [10.228947  ,  0.        ,  2.1525338 , ..., 15.777907  ,\n",
       "            7.309088  ,  0.        ],\n",
       "          [12.800071  ,  0.        ,  3.3098052 , ..., 17.583748  ,\n",
       "            1.4888493 ,  0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 8.917601  ,  6.9645524 ,  7.9212165 , ..., 17.11494   ,\n",
       "           18.507061  ,  0.        ],\n",
       "          [15.021689  ,  0.4145829 ,  0.        , ..., 19.76791   ,\n",
       "            8.345547  ,  0.        ],\n",
       "          [13.215789  ,  0.        ,  0.        , ..., 17.291405  ,\n",
       "            8.490601  ,  0.        ],\n",
       "          ...,\n",
       "          [ 1.6833113 ,  0.        ,  0.        , ..., 16.654995  ,\n",
       "           10.046336  ,  0.        ],\n",
       "          [ 8.556296  ,  0.        ,  1.9914445 , ..., 16.254389  ,\n",
       "            8.676591  ,  0.        ],\n",
       "          [10.123909  ,  0.        ,  0.        , ..., 15.4792185 ,\n",
       "            2.3067589 ,  1.3838952 ]],\n",
       " \n",
       "         [[ 0.        ,  8.730837  ,  0.3487823 , ..., 18.580149  ,\n",
       "           16.553951  ,  0.        ],\n",
       "          [ 1.3442314 ,  2.038674  ,  0.        , ..., 19.790094  ,\n",
       "            1.3909376 ,  0.        ],\n",
       "          [11.263539  ,  0.        ,  1.4122105 , ..., 19.596277  ,\n",
       "            7.7891326 ,  0.        ],\n",
       "          ...,\n",
       "          [ 8.7914715 ,  0.        ,  1.9241823 , ..., 18.5317    ,\n",
       "            7.2494955 ,  0.        ],\n",
       "          [ 5.2426314 ,  0.        ,  0.        , ..., 17.936386  ,\n",
       "            8.572335  ,  0.        ],\n",
       "          [14.04829   ,  0.        ,  4.6483808 , ..., 17.191566  ,\n",
       "            0.        ,  3.0588794 ]],\n",
       " \n",
       "         [[ 0.        ,  2.9368567 ,  0.85413384, ..., 19.772493  ,\n",
       "           21.9378    ,  0.        ],\n",
       "          [ 0.        ,  5.6031585 ,  0.        , ..., 15.693292  ,\n",
       "           10.828394  ,  0.4310508 ],\n",
       "          [13.770744  ,  0.        ,  4.8808875 , ..., 16.119923  ,\n",
       "           14.1771145 ,  0.        ],\n",
       "          ...,\n",
       "          [ 7.46768   ,  0.8755476 ,  1.2320244 , ..., 16.173292  ,\n",
       "           13.283313  ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  4.6540093 , ..., 17.157963  ,\n",
       "           14.09278   ,  0.        ],\n",
       "          [ 3.049763  ,  0.76886195,  2.2314506 , ..., 17.71426   ,\n",
       "            1.9825779 ,  5.8987303 ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 56, 56, 256), dtype=float32, numpy=\n",
       " array([[[[ 1.0724505 ,  0.        ,  0.        , ...,  2.8590755 ,\n",
       "            2.5484886 , 14.89148   ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  3.1462555 ,\n",
       "            0.        , 16.524456  ],\n",
       "          [ 0.        ,  4.064496  ,  0.12738912, ...,  1.6379482 ,\n",
       "            1.4285403 , 14.714208  ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  1.3864162 ,\n",
       "            7.3044724 , 17.905931  ],\n",
       "          [ 0.03528918,  2.6473386 ,  0.        , ...,  1.1415923 ,\n",
       "            1.7764877 , 16.824753  ],\n",
       "          [ 0.        ,  3.6973202 ,  0.        , ...,  1.9878259 ,\n",
       "            0.        , 18.07922   ]],\n",
       " \n",
       "         [[ 0.13516736,  0.        ,  3.4461982 , ...,  3.6642604 ,\n",
       "            7.13763   , 11.226955  ],\n",
       "          [ 0.        ,  0.        ,  5.218773  , ...,  4.8832974 ,\n",
       "            4.2423844 ,  9.563991  ],\n",
       "          [ 1.4671284 ,  0.        ,  8.272515  , ...,  2.6489968 ,\n",
       "            6.095936  ,  9.407639  ],\n",
       "          ...,\n",
       "          [ 2.7741454 ,  0.        ,  1.0916971 , ...,  4.416436  ,\n",
       "            5.2497125 , 11.216367  ],\n",
       "          [ 4.4450903 ,  0.        ,  0.26233616, ...,  2.7816997 ,\n",
       "            0.        ,  8.610506  ],\n",
       "          [ 0.        ,  2.4215717 ,  0.3306996 , ...,  3.485584  ,\n",
       "            0.        , 14.181658  ]],\n",
       " \n",
       "         [[ 0.        ,  0.70310646,  0.        , ...,  2.9924092 ,\n",
       "            3.9104757 ,  7.7710514 ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  1.8809811 ,\n",
       "            0.        ,  3.2275472 ],\n",
       "          [ 4.088106  ,  0.        ,  0.4075226 , ...,  0.97294235,\n",
       "            2.89297   ,  8.627276  ],\n",
       "          ...,\n",
       "          [ 0.6579459 ,  0.        ,  0.        , ...,  3.064707  ,\n",
       "            3.3250785 , 13.609585  ],\n",
       "          [ 3.5648143 ,  0.        ,  0.        , ...,  1.4485459 ,\n",
       "            3.3081262 , 10.725296  ],\n",
       "          [ 0.        ,  0.        ,  2.35276   , ...,  2.9373486 ,\n",
       "            0.        , 16.01666   ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.        ,  2.6033347 ,  0.        , ...,  1.2952167 ,\n",
       "           11.99753   , 12.268688  ],\n",
       "          [ 0.67681175,  1.8004807 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  5.7442293 ],\n",
       "          [ 0.5713258 ,  1.2344973 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  3.520023  ],\n",
       "          ...,\n",
       "          [ 0.99784195,  0.        ,  0.        , ...,  1.3080195 ,\n",
       "            7.865625  ,  8.139332  ],\n",
       "          [ 8.719132  ,  0.07315811,  0.        , ...,  2.3163798 ,\n",
       "            1.4547719 ,  4.661232  ],\n",
       "          [ 0.        ,  1.1682532 ,  0.8724113 , ...,  5.683891  ,\n",
       "            0.        , 12.224572  ]],\n",
       " \n",
       "         [[ 2.3498302 ,  4.8529406 ,  0.78651375, ...,  2.697927  ,\n",
       "           18.076384  , 12.004936  ],\n",
       "          [ 4.070401  ,  3.0441568 ,  1.3360988 , ...,  0.674439  ,\n",
       "            8.67533   ,  5.7573223 ],\n",
       "          [ 4.17693   ,  2.6322315 ,  0.40399328, ...,  0.        ,\n",
       "            0.        ,  3.751257  ],\n",
       "          ...,\n",
       "          [ 6.1957955 ,  6.9995255 ,  0.        , ...,  6.3636055 ,\n",
       "            0.        ,  4.4355497 ],\n",
       "          [ 9.222336  ,  7.9085464 ,  0.        , ...,  6.5925455 ,\n",
       "            0.34507936,  6.0073566 ],\n",
       "          [ 0.        ,  4.1859074 ,  0.        , ...,  5.6425333 ,\n",
       "            0.5809986 , 12.944835  ]],\n",
       " \n",
       "         [[ 0.        ,  5.10897   ,  0.        , ...,  0.        ,\n",
       "           12.402451  ,  6.64299   ],\n",
       "          [ 0.99516904,  3.7425513 ,  0.        , ...,  0.        ,\n",
       "           17.44042   ,  3.9950464 ],\n",
       "          [ 1.7461305 ,  3.9705174 ,  0.        , ...,  0.        ,\n",
       "           17.85054   ,  5.773312  ],\n",
       "          ...,\n",
       "          [ 1.0844407 ,  5.2066655 ,  0.        , ...,  2.6703725 ,\n",
       "            6.1335926 ,  4.513488  ],\n",
       "          [ 0.11040116,  1.5292373 ,  0.        , ...,  0.36579722,\n",
       "            9.843734  ,  5.3883867 ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  2.273872  ,\n",
       "            9.26444   ,  8.885234  ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 56, 56, 256), dtype=float32, numpy=\n",
       " array([[[[ 5.6591706 ,  0.        ,  4.8332224 , ...,  0.        ,\n",
       "           10.786241  ,  2.2876105 ],\n",
       "          [11.008973  ,  0.        ,  0.28478467, ...,  0.        ,\n",
       "            9.55709   ,  5.7946205 ],\n",
       "          [ 8.122979  ,  0.        ,  1.6970809 , ...,  0.        ,\n",
       "            7.176147  ,  6.216493  ],\n",
       "          ...,\n",
       "          [13.57949   ,  0.        ,  1.8873414 , ...,  0.        ,\n",
       "            7.2617474 ,  0.57696927],\n",
       "          [ 7.1105046 ,  0.        ,  2.633598  , ...,  0.        ,\n",
       "            9.572188  ,  0.71171105],\n",
       "          [ 5.025758  ,  0.        ,  4.466172  , ...,  0.        ,\n",
       "            2.2127705 ,  3.708779  ]],\n",
       " \n",
       "         [[ 0.36599898, 10.916193  ,  1.9126753 , ...,  0.44975066,\n",
       "            9.160019  ,  1.8242402 ],\n",
       "          [ 0.        , 10.675662  ,  0.        , ...,  0.        ,\n",
       "            8.548574  ,  5.2994576 ],\n",
       "          [ 0.        ,  4.447308  ,  0.        , ...,  0.        ,\n",
       "            6.3068275 ,  3.2019455 ],\n",
       "          ...,\n",
       "          [ 0.        ,  5.63732   ,  0.        , ...,  0.        ,\n",
       "            2.202171  ,  0.        ],\n",
       "          [ 0.        ,  7.590041  ,  0.        , ...,  0.        ,\n",
       "            6.708733  ,  0.        ],\n",
       "          [ 0.        ,  7.4128737 ,  2.6810272 , ...,  0.        ,\n",
       "            0.        ,  1.9079362 ]],\n",
       " \n",
       "         [[ 0.        ,  9.255369  ,  4.228491  , ...,  3.150629  ,\n",
       "            8.337569  ,  2.1099846 ],\n",
       "          [ 0.        ,  7.239684  ,  1.2449402 , ...,  1.8045329 ,\n",
       "            6.4411116 ,  0.24834245],\n",
       "          [ 0.        ,  3.257795  ,  0.        , ...,  0.        ,\n",
       "            3.9390495 ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  5.818059  ,  0.        , ...,  0.        ,\n",
       "            0.33054534,  0.        ],\n",
       "          [ 0.        ,  9.469992  ,  0.        , ...,  0.        ,\n",
       "            4.4615226 ,  0.        ],\n",
       "          [ 5.040675  , 10.469559  ,  3.8032417 , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[17.869848  ,  7.842003  ,  2.9445899 , ...,  1.6264579 ,\n",
       "           11.314562  ,  3.2446177 ],\n",
       "          [ 0.        ,  6.150126  ,  0.        , ...,  0.        ,\n",
       "            5.728696  ,  2.3302686 ],\n",
       "          [ 0.        ,  1.4032927 ,  8.46719   , ...,  0.02541752,\n",
       "            3.0165515 ,  0.        ],\n",
       "          ...,\n",
       "          [ 5.832417  ,  4.0544734 ,  0.        , ...,  1.1939534 ,\n",
       "            5.184782  ,  0.        ],\n",
       "          [ 0.        ,  6.3371816 ,  0.        , ...,  0.726725  ,\n",
       "            7.3498325 ,  0.        ],\n",
       "          [ 1.5488964 ,  9.236943  ,  4.9680185 , ...,  0.        ,\n",
       "            1.0539947 ,  0.        ]],\n",
       " \n",
       "         [[ 7.3339415 , 11.499683  ,  3.060922  , ...,  0.94893676,\n",
       "           13.899989  ,  7.0788655 ],\n",
       "          [ 0.        , 12.690083  ,  0.        , ...,  0.95997894,\n",
       "            8.326359  ,  3.296123  ],\n",
       "          [ 0.        ,  9.311992  ,  7.8380785 , ...,  6.0583816 ,\n",
       "            4.618795  ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        , 13.605101  ,  0.        , ...,  0.        ,\n",
       "            2.4436924 ,  0.        ],\n",
       "          [ 0.        , 13.829338  ,  0.        , ...,  0.        ,\n",
       "            5.9382033 ,  0.        ],\n",
       "          [ 7.4408946 , 14.042387  ,  5.6999426 , ...,  0.        ,\n",
       "            0.11350396,  0.        ]],\n",
       " \n",
       "         [[11.152137  ,  8.431857  ,  3.0821164 , ...,  3.0894647 ,\n",
       "           12.169744  ,  5.099322  ],\n",
       "          [12.473351  , 16.132475  ,  2.6673963 , ...,  6.561118  ,\n",
       "            7.113584  ,  1.1634898 ],\n",
       "          [17.492989  , 18.689423  ,  6.3530674 , ...,  9.0680895 ,\n",
       "            3.7742383 ,  0.        ],\n",
       "          ...,\n",
       "          [ 5.519507  , 13.79834   ,  3.933718  , ...,  2.707025  ,\n",
       "            6.7792115 ,  0.        ],\n",
       "          [ 2.3211942 , 16.521517  ,  0.        , ...,  6.21234   ,\n",
       "            6.9689703 ,  0.        ],\n",
       "          [10.533691  , 14.310897  ,  5.3435726 , ...,  2.893125  ,\n",
       "            2.1589897 ,  0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 56, 56, 256), dtype=float32, numpy=\n",
       " array([[[[ 8.486716  ,  7.1097755 ,  7.2653217 , ..., 10.115625  ,\n",
       "           12.024276  ,  5.575973  ],\n",
       "          [ 5.181343  , 15.51216   , 11.354811  , ..., 13.2673435 ,\n",
       "           10.591876  ,  3.478726  ],\n",
       "          [10.121176  , 17.228472  , 12.182232  , ..., 15.346921  ,\n",
       "            3.7755919 ,  0.77112997],\n",
       "          ...,\n",
       "          [11.87267   ,  6.802109  , 15.178636  , ..., 10.308559  ,\n",
       "            1.7588882 ,  0.5448592 ],\n",
       "          [11.829205  , 11.790053  , 17.829693  , ...,  9.119651  ,\n",
       "            9.33986   ,  3.6874537 ],\n",
       "          [ 9.935438  , 10.635595  , 11.671086  , ...,  8.836053  ,\n",
       "           10.356961  ,  2.4867444 ]],\n",
       " \n",
       "         [[ 6.708798  , 14.652343  ,  8.115095  , ...,  8.805906  ,\n",
       "           16.612125  ,  4.104922  ],\n",
       "          [ 3.4321535 , 22.477777  ,  9.5194435 , ..., 11.14828   ,\n",
       "           14.34191   ,  2.9653504 ],\n",
       "          [ 3.6376135 , 20.952728  ,  7.104464  , ..., 12.404559  ,\n",
       "            4.648489  ,  1.1710212 ],\n",
       "          ...,\n",
       "          [ 4.5974607 , 14.490662  ,  9.203766  , ...,  6.2334437 ,\n",
       "            5.8613777 ,  1.0610607 ],\n",
       "          [ 7.19975   , 21.820578  , 13.23799   , ...,  5.4348607 ,\n",
       "           13.95373   ,  6.991349  ],\n",
       "          [ 8.742402  , 17.252424  ,  9.930585  , ...,  8.128818  ,\n",
       "           15.342572  ,  3.7934837 ]],\n",
       " \n",
       "         [[10.11006   , 13.670424  ,  6.759289  , ..., 12.536703  ,\n",
       "           15.433944  ,  1.3806598 ],\n",
       "          [ 3.672493  , 12.940222  ,  6.2868433 , ..., 17.10519   ,\n",
       "           11.6651945 ,  0.        ],\n",
       "          [ 0.        , 10.938073  ,  4.1369014 , ..., 17.152767  ,\n",
       "            3.0155134 ,  0.        ],\n",
       "          ...,\n",
       "          [ 3.3688717 , 13.406032  ,  6.960969  , ...,  8.951187  ,\n",
       "            6.4478984 ,  0.        ],\n",
       "          [ 3.481285  , 17.48515   , 11.015159  , ...,  8.882457  ,\n",
       "            7.4431424 ,  3.1873212 ],\n",
       "          [ 4.5407043 , 10.704815  ,  9.202507  , ..., 10.874866  ,\n",
       "            9.412509  ,  0.54141426]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[11.2432    , 13.202687  ,  6.5427527 , ..., 11.156137  ,\n",
       "            5.9368916 ,  3.2059922 ],\n",
       "          [ 9.169013  , 20.095495  ,  9.620629  , ..., 11.936491  ,\n",
       "            4.0394106 ,  1.6102995 ],\n",
       "          [13.1275    , 15.325906  ,  7.491531  , ..., 11.325379  ,\n",
       "            1.0277582 ,  0.        ],\n",
       "          ...,\n",
       "          [ 1.0227412 ,  7.1622567 ,  9.766587  , ...,  8.140473  ,\n",
       "            5.8047767 ,  0.        ],\n",
       "          [ 2.5030746 , 11.453378  ,  9.29003   , ...,  9.294123  ,\n",
       "            7.0518117 ,  1.7260633 ],\n",
       "          [ 4.863358  , 12.7166195 ,  7.4597588 , ..., 13.211286  ,\n",
       "            9.136002  ,  0.        ]],\n",
       " \n",
       "         [[15.068151  , 18.390116  , 11.611558  , ..., 10.8093195 ,\n",
       "            9.302739  ,  3.947813  ],\n",
       "          [ 9.012711  , 28.232674  , 15.835513  , ..., 12.839326  ,\n",
       "            6.513337  ,  2.7251296 ],\n",
       "          [13.537377  , 20.710466  , 13.035422  , ..., 13.475895  ,\n",
       "            1.2268572 ,  0.        ],\n",
       "          ...,\n",
       "          [ 7.514075  , 17.665707  ,  7.1653166 , ..., 11.398828  ,\n",
       "           10.282923  ,  0.        ],\n",
       "          [ 3.7241147 , 22.76781   ,  6.922763  , ..., 11.231818  ,\n",
       "           11.773803  ,  2.7344983 ],\n",
       "          [ 5.555804  , 17.464874  ,  7.0174346 , ..., 13.428968  ,\n",
       "            9.966162  ,  0.        ]],\n",
       " \n",
       "         [[16.91403   , 13.8016405 ,  8.011495  , ...,  9.897416  ,\n",
       "           10.409923  ,  4.7540717 ],\n",
       "          [14.395821  , 22.776447  , 11.701619  , ..., 12.435048  ,\n",
       "            7.135779  ,  2.2862349 ],\n",
       "          [19.229782  , 17.305998  , 11.467454  , ..., 12.925638  ,\n",
       "            2.1810079 ,  0.        ],\n",
       "          ...,\n",
       "          [12.884461  , 19.018991  , 10.957905  , ..., 11.240228  ,\n",
       "            9.141342  ,  0.        ],\n",
       "          [ 3.646384  , 21.648344  , 10.590733  , ...,  9.805717  ,\n",
       "           11.485857  ,  0.        ],\n",
       "          [ 4.496705  , 13.527191  ,  8.60908   , ..., 10.287234  ,\n",
       "            9.158128  ,  0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 56, 56, 256), dtype=float32, numpy=\n",
       " array([[[[ 6.7381067 , 18.943247  ,  2.133404  , ..., 11.131268  ,\n",
       "            0.        , 12.575794  ],\n",
       "          [17.813883  , 56.330963  ,  0.        , ...,  0.        ,\n",
       "            0.        , 16.09816   ],\n",
       "          [27.279337  , 63.426533  ,  0.        , ...,  0.        ,\n",
       "            0.        , 10.220262  ],\n",
       "          ...,\n",
       "          [21.606783  , 55.657608  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  7.065985  ],\n",
       "          [22.637276  , 56.936943  ,  0.        , ...,  1.0746167 ,\n",
       "            0.        , 10.2492485 ],\n",
       "          [18.798521  , 72.34458   ,  0.        , ...,  0.        ,\n",
       "            0.        ,  3.566785  ]],\n",
       " \n",
       "         [[ 2.2507727 , 10.363737  , 16.543055  , ..., 28.687906  ,\n",
       "            1.3602327 , 13.174263  ],\n",
       "          [14.766546  , 44.77126   , 16.745104  , ..., 11.5938225 ,\n",
       "            0.        ,  5.811515  ],\n",
       "          [24.301529  , 42.653923  , 18.41347   , ..., 11.956411  ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [10.040199  , 30.534847  , 16.733604  , ...,  4.5154533 ,\n",
       "            0.        ,  3.5422866 ],\n",
       "          [13.81711   , 35.625725  , 18.180508  , ...,  9.161599  ,\n",
       "            0.        ,  4.115902  ],\n",
       "          [14.542346  , 73.5442    , 11.276332  , ...,  3.5487545 ,\n",
       "            0.        ,  1.0143712 ]],\n",
       " \n",
       "         [[ 6.205577  , 15.676305  , 13.192765  , ..., 25.308725  ,\n",
       "            0.15496571, 12.769604  ],\n",
       "          [21.623943  , 50.710735  , 10.927591  , ...,  5.9942575 ,\n",
       "            0.        ,  3.9156694 ],\n",
       "          [26.229532  , 43.20538   ,  9.283194  , ..., 11.320088  ,\n",
       "            0.        ,  2.8919892 ],\n",
       "          ...,\n",
       "          [12.269688  , 26.927265  , 14.183017  , ...,  4.415706  ,\n",
       "            0.        ,  3.8591468 ],\n",
       "          [15.772549  , 30.330912  , 18.749641  , ..., 13.227408  ,\n",
       "            0.        ,  3.2821658 ],\n",
       "          [15.541576  , 75.62589   , 12.860424  , ..., 10.806095  ,\n",
       "            0.        ,  0.49686748]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[13.24681   , 17.338068  , 14.173778  , ..., 24.291103  ,\n",
       "            0.        , 11.361641  ],\n",
       "          [27.25671   , 54.537796  , 12.566297  , ..., 14.41449   ,\n",
       "            0.        ,  5.7924004 ],\n",
       "          [18.499119  , 42.242504  ,  6.0546093 , ..., 22.89243   ,\n",
       "            0.        ,  0.5550038 ],\n",
       "          ...,\n",
       "          [28.342117  , 33.016758  ,  0.97104603, ...,  0.        ,\n",
       "            0.        , 15.499111  ],\n",
       "          [24.297636  , 30.4858    ,  6.9497805 , ...,  6.918269  ,\n",
       "            0.        , 10.781813  ],\n",
       "          [15.053714  , 67.9749    ,  7.4216733 , ..., 13.155033  ,\n",
       "            0.        ,  7.647052  ]],\n",
       " \n",
       "         [[19.187155  , 16.248661  , 11.663476  , ..., 18.890173  ,\n",
       "            5.4482694 ,  8.067598  ],\n",
       "          [37.54223   , 60.67285   ,  9.944233  , ..., 12.479317  ,\n",
       "            0.        ,  0.6389398 ],\n",
       "          [26.12218   , 57.343372  ,  1.8577278 , ..., 19.63158   ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [38.30754   , 39.249084  ,  1.4002517 , ...,  0.        ,\n",
       "            0.        ,  7.201591  ],\n",
       "          [30.325342  , 40.73832   ,  5.944048  , ...,  5.786472  ,\n",
       "            0.        ,  3.7472126 ],\n",
       "          [17.498781  , 69.65604   ,  4.8375363 , ..., 10.816102  ,\n",
       "            0.        ,  1.9305894 ]],\n",
       " \n",
       "         [[ 2.9496722 ,  1.5643203 , 14.88421   , ..., 19.311092  ,\n",
       "            5.1912026 ,  0.        ],\n",
       "          [10.004887  , 24.643625  , 20.107227  , ..., 21.351116  ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 2.9175556 , 26.591482  , 16.141817  , ..., 29.139103  ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [13.744823  , 15.081755  ,  9.484886  , ...,  2.3788307 ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 9.849993  , 16.764658  , 13.618736  , ..., 13.242491  ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 5.257769  , 35.42296   ,  9.733094  , ..., 11.638043  ,\n",
       "            0.        ,  0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 28, 28, 256), dtype=float32, numpy=\n",
       " array([[[[17.813883  , 56.330963  , 16.745104  , ..., 28.687906  ,\n",
       "            1.3602327 , 16.09816   ],\n",
       "          [29.571423  , 64.998055  , 18.41347   , ..., 11.956411  ,\n",
       "            0.        , 10.220262  ],\n",
       "          [24.161041  , 63.878864  , 17.20058   , ...,  3.8144724 ,\n",
       "            0.        , 12.172622  ],\n",
       "          ...,\n",
       "          [28.745235  , 54.113228  , 11.333634  , ...,  0.        ,\n",
       "            0.        ,  0.48314673],\n",
       "          [22.459005  , 57.290916  , 16.733604  , ...,  4.5154533 ,\n",
       "            0.        ,  7.065985  ],\n",
       "          [22.637276  , 73.5442    , 18.180508  , ...,  9.161599  ,\n",
       "            0.        , 10.2492485 ]],\n",
       " \n",
       "         [[21.623943  , 52.146847  , 13.192765  , ..., 25.308725  ,\n",
       "            0.15496571, 12.769604  ],\n",
       "          [26.229532  , 44.00274   ,  9.283194  , ..., 14.589349  ,\n",
       "            0.        ,  4.9661202 ],\n",
       "          [13.240834  , 26.65012   ,  5.0199857 , ...,  8.915548  ,\n",
       "            0.        ,  6.381545  ],\n",
       "          ...,\n",
       "          [25.87976   , 35.241955  ,  6.3420215 , ...,  0.        ,\n",
       "            0.        , 14.4195    ],\n",
       "          [22.845102  , 37.330635  , 14.183017  , ...,  4.415706  ,\n",
       "            0.        ,  7.664631  ],\n",
       "          [15.772549  , 75.62589   , 18.749641  , ..., 13.227408  ,\n",
       "            0.        ,  3.2821658 ]],\n",
       " \n",
       "         [[17.781487  , 50.15247   ,  8.42248   , ..., 18.634527  ,\n",
       "            0.340823  ,  0.        ],\n",
       "          [25.070332  , 43.003033  ,  3.768952  , ..., 13.285662  ,\n",
       "            0.        ,  0.        ],\n",
       "          [17.769575  , 32.745605  ,  2.2698286 , ..., 12.574993  ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [27.832293  , 34.034252  ,  2.8747208 , ...,  3.1616006 ,\n",
       "            0.        ,  4.00397   ],\n",
       "          [29.848228  , 38.177803  , 10.403729  , ...,  3.5984967 ,\n",
       "            0.        , 11.048654  ],\n",
       "          [13.98592   , 74.251656  , 16.098528  , ..., 18.611685  ,\n",
       "            0.        ,  6.9622674 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[22.87519   , 50.018967  ,  9.849535  , ..., 20.313206  ,\n",
       "            0.        ,  6.215234  ],\n",
       "          [17.828852  , 40.93767   ,  8.754408  , ..., 13.194545  ,\n",
       "            0.        ,  8.374133  ],\n",
       "          [11.303257  , 35.784557  ,  4.6383224 , ..., 18.162342  ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [22.463451  , 44.288597  ,  2.8884137 , ...,  6.8823547 ,\n",
       "            0.        ,  5.224163  ],\n",
       "          [25.735308  , 42.81549   ,  0.        , ...,  0.        ,\n",
       "            0.        ,  2.0598311 ],\n",
       "          [37.508213  , 75.95254   ,  6.4397087 , ..., 11.544415  ,\n",
       "            0.        ,  0.8514194 ]],\n",
       " \n",
       "         [[27.25671   , 54.537796  , 14.173778  , ..., 25.373056  ,\n",
       "            0.        , 12.400526  ],\n",
       "          [18.499119  , 42.242504  ,  8.512337  , ..., 22.89243   ,\n",
       "            0.        ,  3.862498  ],\n",
       "          [32.885868  , 35.35738   ,  0.        , ..., 15.634536  ,\n",
       "            0.        ,  9.410898  ],\n",
       "          ...,\n",
       "          [28.931494  , 44.26223   ,  3.4186854 , ...,  6.0175495 ,\n",
       "            0.        , 13.896291  ],\n",
       "          [29.233843  , 44.05859   ,  0.97104603, ...,  0.97890365,\n",
       "            0.        , 19.079016  ],\n",
       "          [26.312565  , 70.42743   ,  7.4216733 , ..., 13.155033  ,\n",
       "            0.        , 10.781813  ]],\n",
       " \n",
       "         [[37.54223   , 60.67285   , 20.107227  , ..., 21.351116  ,\n",
       "            5.4482694 ,  8.067598  ],\n",
       "          [26.12218   , 57.343372  , 16.141817  , ..., 29.139103  ,\n",
       "            4.381312  ,  0.        ],\n",
       "          [39.023907  , 49.84243   ,  5.2659364 , ..., 12.812657  ,\n",
       "            4.452644  ,  6.054871  ],\n",
       "          ...,\n",
       "          [39.199303  , 43.2114    ,  5.8115053 , ...,  0.5009269 ,\n",
       "            0.        , 20.704325  ],\n",
       "          [40.58131   , 41.582287  ,  9.484886  , ...,  2.3788307 ,\n",
       "            0.        , 15.057766  ],\n",
       "          [30.325342  , 69.65604   , 13.618736  , ..., 13.242491  ,\n",
       "            0.        ,  3.7472126 ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 28, 28, 512), dtype=float32, numpy=\n",
       " array([[[[9.13864040e+00, 5.05263615e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 3.71488190e+01, 0.00000000e+00],\n",
       "          [6.26584962e-02, 1.01370068e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 4.90042915e+01, 0.00000000e+00],\n",
       "          [1.41467104e+01, 1.59362230e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 5.48741493e+01, 0.00000000e+00],\n",
       "          ...,\n",
       "          [1.28638229e+01, 1.54751959e+01, 3.22214603e-01, ...,\n",
       "           0.00000000e+00, 5.06392021e+01, 0.00000000e+00],\n",
       "          [1.20993576e+01, 1.57444830e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 5.01941948e+01, 0.00000000e+00],\n",
       "          [4.38389015e+00, 1.50550470e+01, 7.04647636e+00, ...,\n",
       "           0.00000000e+00, 4.41550713e+01, 0.00000000e+00]],\n",
       " \n",
       "         [[2.73419785e+00, 2.43500957e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 2.43044415e+01, 0.00000000e+00],\n",
       "          [0.00000000e+00, 4.02227097e+01, 6.31274557e+00, ...,\n",
       "           0.00000000e+00, 1.99019375e+01, 0.00000000e+00],\n",
       "          [4.16594744e+00, 4.26167068e+01, 1.10568628e+01, ...,\n",
       "           0.00000000e+00, 2.63466434e+01, 0.00000000e+00],\n",
       "          ...,\n",
       "          [3.59591103e+00, 2.44485359e+01, 1.44277611e+01, ...,\n",
       "           0.00000000e+00, 1.84494228e+01, 0.00000000e+00],\n",
       "          [3.72670949e-01, 2.18798027e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 4.68746185e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 1.81531029e+01, 1.94279289e+01, ...,\n",
       "           0.00000000e+00, 5.47722387e+00, 0.00000000e+00]],\n",
       " \n",
       "         [[3.10734797e+00, 2.00904217e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 1.00140381e+01, 5.58224869e+00],\n",
       "          [3.40388966e+00, 3.59557343e+01, 4.39051199e+00, ...,\n",
       "           0.00000000e+00, 9.70374775e+00, 0.00000000e+00],\n",
       "          [4.63032103e+00, 3.94904213e+01, 1.46239138e+00, ...,\n",
       "           0.00000000e+00, 1.83044720e+01, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 2.92110901e+01, 6.20568752e-01, ...,\n",
       "           0.00000000e+00, 1.65423603e+01, 0.00000000e+00],\n",
       "          [0.00000000e+00, 2.74459496e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 1.92455235e+01, 0.00000000e+00],\n",
       "          [0.00000000e+00, 2.13315830e+01, 1.11356840e+01, ...,\n",
       "           0.00000000e+00, 2.13480816e+01, 4.11785221e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[7.21815443e+00, 1.21423969e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 4.71737289e+00, 1.06105032e+01],\n",
       "          [0.00000000e+00, 2.48173466e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 1.73582292e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 2.31621342e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 1.40713825e+01, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 2.50212746e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 2.00568657e+01, 0.00000000e+00],\n",
       "          [0.00000000e+00, 2.14923534e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 1.58056297e+01, 0.00000000e+00],\n",
       "          [3.77970529e+00, 1.85803547e+01, 1.86973286e+01, ...,\n",
       "           0.00000000e+00, 1.48349676e+01, 2.85275745e+00]],\n",
       " \n",
       "         [[6.33503008e+00, 1.34175072e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 1.76284962e+01, 1.20372047e+01],\n",
       "          [0.00000000e+00, 2.57096138e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 1.36193600e+01, 0.00000000e+00],\n",
       "          [0.00000000e+00, 2.46614208e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 2.13917484e+01, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 3.17997818e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 3.39267502e+01, 0.00000000e+00],\n",
       "          [0.00000000e+00, 3.09038200e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 2.69871254e+01, 0.00000000e+00],\n",
       "          [2.33346868e+00, 2.68652248e+01, 1.74018593e+01, ...,\n",
       "           0.00000000e+00, 2.35073586e+01, 0.00000000e+00]],\n",
       " \n",
       "         [[1.39139385e+01, 3.07371559e+01, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 3.33088875e+01],\n",
       "          [6.63198137e+00, 4.89656868e+01, 2.40653362e+01, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 1.87669144e+01],\n",
       "          [5.58495855e+00, 5.16052742e+01, 2.11401596e+01, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 1.46119757e+01],\n",
       "          ...,\n",
       "          [0.00000000e+00, 6.29503517e+01, 2.93603535e+01, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 1.59443302e+01],\n",
       "          [5.92673683e+00, 6.26136208e+01, 2.39458294e+01, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 1.29015808e+01],\n",
       "          [1.03381042e+01, 4.60410004e+01, 3.12620544e+01, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 1.14872208e+01]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 28, 28, 512), dtype=float32, numpy=\n",
       " array([[[[ 3.300012 ,  0.       ,  0.       , ..., 16.50094  ,\n",
       "            7.0820475,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ...,  9.934669 ,\n",
       "           15.476985 ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ...,  2.8748517,\n",
       "            8.847964 ,  0.       ],\n",
       "          ...,\n",
       "          [ 0.       ,  0.       ,  0.       , ...,  5.6180415,\n",
       "            5.2703137,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ...,  6.154913 ,\n",
       "            1.7975911,  0.       ],\n",
       "          [ 0.       ,  0.       ,  2.0707996, ..., 13.452665 ,\n",
       "            7.1731806,  4.4315643]],\n",
       " \n",
       "         [[ 0.       ,  0.       ,  0.       , ..., 24.095413 ,\n",
       "            2.4434402,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 25.249182 ,\n",
       "           16.652845 ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 20.21677  ,\n",
       "           13.006414 ,  0.       ],\n",
       "          ...,\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 17.9602   ,\n",
       "            2.2070596,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 24.049871 ,\n",
       "            0.       ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 33.81783  ,\n",
       "            4.561946 ,  3.171565 ]],\n",
       " \n",
       "         [[ 0.       ,  0.       ,  0.       , ..., 16.48171  ,\n",
       "            1.7923013,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 16.836363 ,\n",
       "            6.89324  ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 12.308185 ,\n",
       "            3.096894 ,  0.       ],\n",
       "          ...,\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 18.141813 ,\n",
       "            0.       ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 26.94126  ,\n",
       "            0.       ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 36.577496 ,\n",
       "            2.6570878,  0.       ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.       ,  0.       ,  0.       , ..., 27.239023 ,\n",
       "           16.117287 ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 24.456524 ,\n",
       "           17.282387 ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 21.631823 ,\n",
       "           23.535332 ,  0.       ],\n",
       "          ...,\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 12.567086 ,\n",
       "           16.382805 ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 13.309746 ,\n",
       "            1.6719893,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.1527943, ..., 29.318798 ,\n",
       "            5.0942497,  0.       ]],\n",
       " \n",
       "         [[ 0.       ,  0.       ,  0.       , ..., 19.053036 ,\n",
       "           23.367954 ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 17.007551 ,\n",
       "           29.13558  ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 14.842173 ,\n",
       "           20.41677  ,  0.       ],\n",
       "          ...,\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 13.140267 ,\n",
       "           15.255698 ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 14.225298 ,\n",
       "            0.       ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 31.399178 ,\n",
       "            5.267046 ,  1.5308121]],\n",
       " \n",
       "         [[ 9.453194 ,  0.       ,  0.       , ...,  8.898208 ,\n",
       "           17.950085 ,  8.238776 ],\n",
       "          [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "           28.762901 ,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ...,  4.4570394,\n",
       "           16.23747  ,  0.       ],\n",
       "          ...,\n",
       "          [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "            3.3047595,  0.       ],\n",
       "          [ 0.       ,  0.       ,  0.       , ...,  6.4576607,\n",
       "            0.       ,  3.7913816],\n",
       "          [ 0.       ,  0.       ,  0.       , ..., 16.779034 ,\n",
       "            0.       ,  3.4374807]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 28, 28, 512), dtype=float32, numpy=\n",
       " array([[[[ 0.        , 10.38194   ,  8.618009  , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  1.3884116 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        , 12.73575   , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        , 46.296005  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 47.312103  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 40.518154  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        , 17.688528  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 16.927795  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 15.355759  ,  6.824188  , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        , 36.266747  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 35.56783   ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 27.7749    ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        , 23.040792  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 26.269962  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 26.366201  ,  6.3267627 , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.        , 25.921085  ,  0.        , ...,  0.        ,\n",
       "           11.634803  ,  0.        ],\n",
       "          [ 0.        , 26.274515  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 23.036425  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        , 16.013195  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 22.580883  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 25.541931  ,  1.9453354 , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        , 29.29364   ,  0.        , ...,  0.        ,\n",
       "            1.6531541 ,  0.        ],\n",
       "          [ 0.        , 24.900845  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 23.614393  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        , 14.029961  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 27.07617   ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 35.638046  ,  5.3319745 , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 4.322586  , 33.7474    ,  0.        , ...,  0.        ,\n",
       "           12.452464  ,  6.4999123 ],\n",
       "          [ 0.        , 30.431814  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 26.424225  ,  0.        , ...,  0.        ,\n",
       "            5.6584272 ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        , 23.93803   ,  0.        , ...,  0.        ,\n",
       "            3.965392  ,  0.        ],\n",
       "          [ 0.        , 37.02001   ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 43.923378  ,  0.        , ...,  0.        ,\n",
       "            6.1718454 ,  0.38964128]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 28, 28, 512), dtype=float32, numpy=\n",
       " array([[[[ 0.        ,  9.1528635 ,  0.        , ...,  0.        ,\n",
       "            0.7856107 ,  0.        ],\n",
       "          [ 0.        ,  7.368189  ,  1.0954173 , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  6.171067  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  3.7304454 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  1.9118811 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  3.1350641 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  5.830378  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  7.2149444 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  4.9919715 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  6.9763765 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 9.160148  ,  8.006862  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 7.536074  , 16.74631   ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.39287186, ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 6.2376976 ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 6.168863  , 10.551387  ,  0.        , ...,  0.        ,\n",
       "            0.17819971,  0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.32097524,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        , 12.262512  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  6.3373375 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.991562  ,  0.        ,  1.1138111 , ...,  0.        ,\n",
       "            5.757203  ,  0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 14, 14, 512), dtype=float32, numpy=\n",
       " array([[[[ 0.        ,  9.1528635 ,  1.0954173 , ...,  0.        ,\n",
       "            0.7856107 ,  0.        ],\n",
       "          [ 0.        ,  6.3917804 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  6.359369  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  7.2445474 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  6.9763765 ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 9.160148  , 16.74631   ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.39287186, ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.17374337, ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 6.2376976 , 10.551387  ,  0.        , ...,  0.        ,\n",
       "            0.17819971,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.24506027, ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  1.729143  , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            1.2429712 ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.90137047,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.14669928, ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            2.3704271 ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.9571617 ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 2.6635466 ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.49572524, ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.516843  ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.10367589,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.32097524,  0.        ],\n",
       "          [ 0.23889987, 12.262512  ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ]],\n",
       " \n",
       "         [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          ...,\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "            0.        ,  0.        ],\n",
       "          [ 0.991562  ,  6.3373375 ,  1.1138111 , ...,  0.        ,\n",
       "            5.757203  ,  0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 14, 14, 512), dtype=float32, numpy=\n",
       " array([[[[0.3099896 , 0.        , 4.030868  , ..., 0.17023884,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.6932307 ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.6814325 ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 1.5957735 ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.19482577,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[1.3179209 , 0.        , 3.2879136 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 1.3660176 ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.23424162,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.79061747, 0.        , 0.3180975 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.0537666 ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.        , 0.76200026, ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 4.2875996 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[2.6456413 , 0.        , 3.6052291 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.40737265, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.5824409 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 14, 14, 512), dtype=float32, numpy=\n",
       " array([[[[0.        , 0.        , 4.4308043 , ..., 0.        ,\n",
       "           3.1767657 , 0.81386405],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           4.104527  , 0.79647374],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           3.8476832 , 0.80629355],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           4.0481076 , 0.39740235],\n",
       "          [0.        , 0.        , 0.30776662, ..., 0.        ,\n",
       "           4.1022067 , 0.6796449 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           2.8901615 , 0.33805323]],\n",
       " \n",
       "         [[0.        , 0.        , 4.276568  , ..., 0.        ,\n",
       "           0.        , 0.4551269 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.21453209],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.5351188 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           1.225212  , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.94453144, 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.4278597 , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 4.0981793 , ..., 0.9933131 ,\n",
       "           0.        , 1.2291107 ],\n",
       "          [0.        , 0.        , 0.        , ..., 1.2323903 ,\n",
       "           0.        , 0.9378151 ],\n",
       "          [0.        , 0.        , 0.        , ..., 1.9789597 ,\n",
       "           0.        , 0.9634089 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 2.0105586 ,\n",
       "           0.58890784, 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 1.1464573 ,\n",
       "           0.42830935, 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.21621561,\n",
       "           0.08729146, 0.61767256]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.        , 4.114144  , ..., 0.        ,\n",
       "           0.        , 1.3153719 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 2.1710253 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.08986318,\n",
       "           0.        , 1.8123653 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.4217894 ,\n",
       "           0.        , 1.0859731 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.00945397, 0.9796043 ]],\n",
       " \n",
       "         [[0.        , 0.        , 4.1570163 , ..., 0.        ,\n",
       "           0.        , 1.0800043 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 1.2311335 ],\n",
       "          [0.15746738, 0.        , 0.        , ..., 0.3201506 ,\n",
       "           0.        , 0.9491538 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.04171826, 0.2510118 ]],\n",
       " \n",
       "         [[0.        , 0.        , 3.352816  , ..., 0.70594114,\n",
       "           0.        , 1.1504714 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.8814138 ,\n",
       "           0.        , 1.1011113 ],\n",
       "          [0.        , 0.        , 0.        , ..., 1.4946569 ,\n",
       "           0.        , 0.77901024],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.7672099 ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.35790026,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.9671246 , ..., 0.        ,\n",
       "           0.        , 0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 14, 14, 512), dtype=float32, numpy=\n",
       " array([[[[0.        , 0.        , 0.00213616, ..., 0.4000792 ,\n",
       "           0.        , 0.0912486 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.10591354,\n",
       "           0.        , 0.2298481 ],\n",
       "          [0.        , 0.        , 0.17158742, ..., 0.07921255,\n",
       "           0.        , 0.22478567],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.2558956 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.07542834, ..., 0.19662306,\n",
       "           0.28438938, 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.36282018],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.83110106],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.36663666],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.1029326 , 0.23060834]],\n",
       " \n",
       "         [[0.        , 0.        , 0.89852816, ..., 0.        ,\n",
       "           0.        , 0.3208703 ],\n",
       "          [0.        , 0.        , 0.5476272 , ..., 0.        ,\n",
       "           0.        , 0.3754825 ],\n",
       "          [0.        , 0.        , 1.007014  , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.5070537 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.621348  , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.0850548 , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.        , 0.86137396, ..., 0.        ,\n",
       "           0.        , 0.00423042],\n",
       "          [0.        , 0.        , 0.48694447, ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.90319407, ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.6767452 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 1.0434234 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.23791945, ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 1.1666818 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.9070336 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 1.158663  , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.7008961 , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.79507697, ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.11346683, 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.53213197, ..., 0.4175708 ,\n",
       "           0.20516229, 0.40622196],\n",
       "          [0.        , 0.        , 0.00906968, ..., 0.        ,\n",
       "           0.26490068, 0.6935953 ],\n",
       "          [0.        , 0.        , 0.16523185, ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          ...,\n",
       "          [0.08416085, 0.        , 0.        , ..., 0.        ,\n",
       "           0.02435815, 0.        ],\n",
       "          [0.02256725, 0.        , 0.        , ..., 0.        ,\n",
       "           0.42172933, 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.20381834,\n",
       "           0.7409303 , 0.18935512]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 14, 14, 512), dtype=float32, numpy=\n",
       " array([[[[0.38305   , 0.        , 0.        , ..., 0.        ,\n",
       "           1.019628  , 0.        ],\n",
       "          [0.5017806 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.59685457, 0.        ],\n",
       "          [0.44479537, 0.        , 0.        , ..., 0.        ,\n",
       "           0.6779884 , 0.        ],\n",
       "          ...,\n",
       "          [0.42429942, 0.        , 0.        , ..., 0.        ,\n",
       "           0.8016232 , 0.        ],\n",
       "          [0.57528573, 0.        , 0.        , ..., 0.        ,\n",
       "           0.6188441 , 0.        ],\n",
       "          [0.45175278, 0.        , 0.        , ..., 0.        ,\n",
       "           0.6482715 , 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.97404456, 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.372322  , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.4425824 , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.4988488 , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.2328183 , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.45490417, 0.        ]],\n",
       " \n",
       "         [[0.01675832, 0.        , 0.        , ..., 0.        ,\n",
       "           1.1882104 , 0.        ],\n",
       "          [0.10356379, 0.        , 0.        , ..., 0.        ,\n",
       "           0.6847949 , 0.        ],\n",
       "          [0.01355541, 0.        , 0.        , ..., 0.        ,\n",
       "           0.7234916 , 0.        ],\n",
       "          ...,\n",
       "          [0.0555495 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.81348014, 0.        ],\n",
       "          [0.25170165, 0.        , 0.        , ..., 0.        ,\n",
       "           0.6259417 , 0.        ],\n",
       "          [0.22828656, 0.        , 0.        , ..., 0.        ,\n",
       "           0.7020932 , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           1.422122  , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.83765906, 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.8443912 , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.88048726, 0.        ],\n",
       "          [0.076877  , 0.        , 0.        , ..., 0.        ,\n",
       "           0.71711874, 0.        ],\n",
       "          [0.00522381, 0.        , 0.        , ..., 0.        ,\n",
       "           0.81013143, 0.        ]],\n",
       " \n",
       "         [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           1.4009662 , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.8481052 , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           1.0213072 , 0.        ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.79304576, 0.        ],\n",
       "          [0.04360259, 0.        , 0.        , ..., 0.        ,\n",
       "           0.5562165 , 0.        ],\n",
       "          [0.03293717, 0.        , 0.        , ..., 0.        ,\n",
       "           0.6546158 , 0.        ]],\n",
       " \n",
       "         [[0.10613585, 0.        , 0.        , ..., 0.        ,\n",
       "           1.4048681 , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           1.0715599 , 0.        ],\n",
       "          [0.0760814 , 0.        , 0.        , ..., 0.        ,\n",
       "           1.1570783 , 0.        ],\n",
       "          ...,\n",
       "          [0.37407127, 0.        , 0.        , ..., 0.        ,\n",
       "           0.7742883 , 0.        ],\n",
       "          [0.46606493, 0.        , 0.        , ..., 0.        ,\n",
       "           0.64104474, 0.        ],\n",
       "          [0.37884206, 0.        , 0.        , ..., 0.        ,\n",
       "           0.6755418 , 0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 7, 7, 512), dtype=float32, numpy=\n",
       " array([[[[0.5017806 , 0.        , 0.        , ..., 0.        ,\n",
       "           1.019628  , 0.        ],\n",
       "          [0.5975069 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.6779884 , 0.        ],\n",
       "          [0.6085675 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.7738501 , 0.        ],\n",
       "          ...,\n",
       "          [0.7572707 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.72167444, 0.        ],\n",
       "          [0.6072013 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.8016232 , 0.        ],\n",
       "          [0.57528573, 0.        , 0.        , ..., 0.        ,\n",
       "           0.6482715 , 0.        ]],\n",
       " \n",
       "         [[0.10356379, 0.        , 0.        , ..., 0.        ,\n",
       "           1.2766917 , 0.        ],\n",
       "          [0.2211476 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.76277715, 0.        ],\n",
       "          [0.28575587, 0.        , 0.        , ..., 0.        ,\n",
       "           0.83011365, 0.        ],\n",
       "          ...,\n",
       "          [0.3214626 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.9534251 , 0.        ],\n",
       "          [0.26294774, 0.        , 0.        , ..., 0.        ,\n",
       "           0.8403452 , 0.        ],\n",
       "          [0.25170165, 0.        , 0.        , ..., 0.        ,\n",
       "           0.7788597 , 0.        ]],\n",
       " \n",
       "         [[0.17371672, 0.        , 0.        , ..., 0.        ,\n",
       "           1.2703304 , 0.        ],\n",
       "          [0.24212852, 0.        , 0.        , ..., 0.        ,\n",
       "           0.7956473 , 0.        ],\n",
       "          [0.27728274, 0.        , 0.        , ..., 0.        ,\n",
       "           0.88191694, 0.        ],\n",
       "          ...,\n",
       "          [0.39845586, 0.        , 0.        , ..., 0.        ,\n",
       "           0.93803346, 0.        ],\n",
       "          [0.399024  , 0.        , 0.        , ..., 0.        ,\n",
       "           0.87879497, 0.        ],\n",
       "          [0.3016825 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.8341601 , 0.        ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.13495594, 0.        , 0.        , ..., 0.        ,\n",
       "           1.2835073 , 0.        ],\n",
       "          [0.33262137, 0.        , 0.        , ..., 0.        ,\n",
       "           0.7731456 , 0.        ],\n",
       "          [0.48476735, 0.        , 0.        , ..., 0.        ,\n",
       "           0.7985604 , 0.        ],\n",
       "          ...,\n",
       "          [0.4981244 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.77405053, 0.        ],\n",
       "          [0.53815186, 0.        , 0.        , ..., 0.        ,\n",
       "           0.7661197 , 0.        ],\n",
       "          [0.37450725, 0.        , 0.        , ..., 0.        ,\n",
       "           0.8122649 , 0.        ]],\n",
       " \n",
       "         [[0.1332376 , 0.        , 0.        , ..., 0.        ,\n",
       "           1.422122  , 0.        ],\n",
       "          [0.12434381, 0.        , 0.        , ..., 0.        ,\n",
       "           0.8443912 , 0.        ],\n",
       "          [0.2259776 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.9099896 , 0.        ],\n",
       "          ...,\n",
       "          [0.32420453, 0.        , 0.        , ..., 0.        ,\n",
       "           0.88070595, 0.        ],\n",
       "          [0.328078  , 0.        , 0.        , ..., 0.        ,\n",
       "           0.88048726, 0.        ],\n",
       "          [0.19399607, 0.        , 0.        , ..., 0.        ,\n",
       "           0.8149733 , 0.        ]],\n",
       " \n",
       "         [[0.10613585, 0.        , 0.        , ..., 0.        ,\n",
       "           1.4048681 , 0.        ],\n",
       "          [0.21385336, 0.        , 0.        , ..., 0.        ,\n",
       "           1.1570783 , 0.        ],\n",
       "          [0.31349096, 0.        , 0.        , ..., 0.        ,\n",
       "           1.0530485 , 0.        ],\n",
       "          ...,\n",
       "          [0.37078094, 0.        , 0.        , ..., 0.        ,\n",
       "           0.9134761 , 0.        ],\n",
       "          [0.3807648 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.8560576 , 0.        ],\n",
       "          [0.46606493, 0.        , 0.        , ..., 0.        ,\n",
       "           0.6755418 , 0.        ]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 25088), dtype=float32, numpy=\n",
       " array([[0.5017806, 0.       , 0.       , ..., 0.       , 0.6755418,\n",
       "         0.       ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4096), dtype=float32, numpy=\n",
       " array([[0.       , 1.7937832, 0.       , ..., 0.       , 2.8067496,\n",
       "         1.2534225]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4096), dtype=float32, numpy=\n",
       " array([[0.56897914, 0.        , 0.47117332, ..., 0.        , 0.53980356,\n",
       "         0.        ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1000), dtype=float32, numpy=\n",
       " array([[1.93909567e-04, 1.30995293e-03, 6.70775829e-04, 1.55657006e-03,\n",
       "         2.31764629e-03, 2.06703180e-03, 8.27700645e-03, 1.93845233e-04,\n",
       "         3.11497686e-04, 2.43857779e-04, 6.68900611e-04, 2.97750637e-04,\n",
       "         4.04240243e-04, 8.72406235e-04, 3.54681746e-04, 3.53287323e-04,\n",
       "         7.34907400e-04, 4.11674380e-04, 7.60196010e-04, 1.16276951e-03,\n",
       "         1.53521763e-03, 9.82385129e-04, 7.28983083e-04, 6.01750275e-04,\n",
       "         1.43636513e-04, 2.61252542e-04, 1.13062619e-03, 1.08018762e-03,\n",
       "         2.86136667e-04, 4.77693416e-03, 3.62332969e-04, 4.08532040e-04,\n",
       "         4.23065707e-04, 1.52014091e-03, 9.96582443e-04, 5.37109619e-04,\n",
       "         1.27231842e-03, 4.03395243e-04, 2.42167362e-03, 2.87171802e-04,\n",
       "         7.05428072e-04, 6.86353305e-04, 1.03421498e-03, 5.68329007e-04,\n",
       "         8.68871051e-04, 2.05612998e-03, 8.96130165e-04, 6.76628784e-04,\n",
       "         5.38379652e-04, 5.60298970e-04, 9.17974161e-04, 3.37562233e-04,\n",
       "         2.66298256e-03, 2.68744724e-03, 8.33312341e-04, 3.03997891e-04,\n",
       "         1.49945915e-03, 2.22484916e-04, 1.09003615e-02, 3.84143670e-04,\n",
       "         1.34824100e-03, 1.03763910e-03, 1.03460136e-03, 9.37898760e-04,\n",
       "         2.58188765e-03, 1.63560268e-03, 2.66647106e-03, 1.08045933e-03,\n",
       "         1.27383368e-03, 2.26026704e-03, 1.53956783e-03, 2.22613802e-03,\n",
       "         6.10234099e-04, 1.12312520e-03, 4.85103228e-04, 5.07041905e-03,\n",
       "         1.35725935e-03, 2.12232326e-03, 9.13768169e-03, 5.10801002e-03,\n",
       "         1.18862628e-03, 5.07185282e-03, 6.16740610e-04, 7.53846951e-04,\n",
       "         3.15499405e-04, 1.79730321e-03, 1.59311155e-03, 2.25196290e-03,\n",
       "         2.54106097e-04, 2.76325061e-03, 8.11420541e-05, 4.28387953e-04,\n",
       "         1.01122295e-03, 1.82692689e-04, 8.03259667e-04, 1.57826449e-04,\n",
       "         4.66782658e-04, 2.43215123e-04, 9.25909670e-04, 7.27780687e-04,\n",
       "         2.95686797e-04, 2.60004570e-04, 3.48574365e-04, 1.94042490e-03,\n",
       "         2.94711615e-04, 9.23410335e-05, 6.85017731e-04, 6.26167166e-04,\n",
       "         5.53824357e-04, 5.32751554e-04, 2.03435658e-04, 3.05487285e-03,\n",
       "         1.29841338e-03, 5.47718839e-04, 6.00129657e-04, 7.32681365e-05,\n",
       "         1.65165999e-04, 1.47700147e-03, 3.47070541e-04, 2.20772476e-04,\n",
       "         2.08209967e-03, 7.65463265e-05, 1.04254599e-04, 9.18922597e-05,\n",
       "         1.24488259e-03, 1.33589271e-03, 2.01192917e-03, 1.08563295e-03,\n",
       "         1.19702867e-03, 9.93845286e-04, 2.46867741e-04, 8.84606678e-04,\n",
       "         2.12714635e-03, 3.34692944e-04, 8.43950082e-04, 2.84347567e-04,\n",
       "         1.83481199e-04, 1.22440315e-03, 2.57104239e-03, 6.44008047e-04,\n",
       "         1.83981413e-03, 1.93855481e-03, 1.55014067e-03, 8.67056835e-04,\n",
       "         4.78311325e-04, 6.59065379e-04, 1.94597989e-03, 1.33741283e-04,\n",
       "         1.88078877e-04, 2.13370635e-03, 7.40112853e-04, 1.40126678e-03,\n",
       "         3.03197186e-04, 4.57949983e-03, 2.19614687e-03, 1.50560460e-03,\n",
       "         6.92480826e-04, 1.02119159e-03, 5.76533785e-04, 2.15639680e-04,\n",
       "         2.78612919e-04, 6.51779177e-04, 1.37149531e-03, 5.48125478e-04,\n",
       "         3.45740205e-04, 9.08071306e-05, 7.43655022e-04, 1.67094593e-04,\n",
       "         4.30061482e-04, 4.47949598e-04, 6.70210458e-04, 1.31755986e-03,\n",
       "         1.73484802e-03, 6.91496825e-04, 5.19057503e-04, 1.59526724e-04,\n",
       "         3.39590799e-04, 2.09289065e-04, 1.01681030e-03, 8.06513475e-04,\n",
       "         7.53871165e-04, 7.08714593e-04, 5.78827807e-04, 8.30628342e-05,\n",
       "         4.60288429e-04, 4.51272674e-04, 3.34690878e-04, 2.71337718e-04,\n",
       "         3.63040803e-04, 2.91374366e-04, 1.22484053e-03, 1.73654887e-04,\n",
       "         1.29785878e-03, 2.41151924e-04, 4.39658616e-04, 4.44342644e-04,\n",
       "         5.31900849e-04, 1.14328825e-04, 1.00140707e-04, 6.99085416e-04,\n",
       "         2.49636883e-04, 1.50899446e-04, 1.03999791e-03, 4.61714761e-03,\n",
       "         1.95171346e-03, 8.85897171e-05, 6.84779807e-05, 7.00603530e-04,\n",
       "         6.78505923e-04, 1.64312834e-03, 2.22218689e-04, 2.94059690e-04,\n",
       "         8.56235216e-04, 1.52632492e-04, 1.29478838e-04, 7.00861448e-04,\n",
       "         3.77123826e-04, 5.42087539e-04, 4.83667391e-04, 2.83842412e-04,\n",
       "         1.10527384e-04, 7.06178835e-05, 1.77464460e-03, 4.02273436e-04,\n",
       "         8.96179554e-05, 2.74843042e-04, 1.93131855e-04, 1.83043128e-04,\n",
       "         4.29429288e-04, 1.15918962e-03, 6.37999503e-04, 2.71420140e-04,\n",
       "         2.60146335e-04, 1.74232337e-04, 1.98181428e-04, 1.50852764e-04,\n",
       "         2.73114129e-04, 1.02837745e-03, 5.89840172e-04, 4.87469893e-04,\n",
       "         1.96087203e-04, 2.60041386e-04, 9.91386361e-04, 3.05449619e-04,\n",
       "         7.72747371e-05, 1.36517733e-03, 3.08666786e-04, 4.03092738e-04,\n",
       "         5.31723490e-04, 5.62290195e-04, 1.53583637e-03, 5.30337857e-04,\n",
       "         1.32323577e-04, 5.14519226e-04, 8.83008819e-04, 8.96077836e-05,\n",
       "         1.32649002e-04, 2.89480458e-03, 7.55111268e-03, 1.40594377e-03,\n",
       "         4.54598892e-04, 3.77135148e-04, 1.16557749e-04, 1.39448920e-03,\n",
       "         5.48709184e-04, 1.66034757e-03, 5.98411250e-04, 2.70956050e-04,\n",
       "         3.84620245e-04, 2.46099662e-04, 1.32598914e-03, 1.47078797e-04,\n",
       "         5.00509166e-04, 9.96019458e-04, 3.59581347e-04, 2.54074112e-04,\n",
       "         6.41315943e-04, 6.31947361e-04, 4.79290204e-04, 5.04042255e-03,\n",
       "         4.34124202e-04, 4.86631820e-04, 7.35342444e-04, 3.74251278e-03,\n",
       "         3.93883279e-03, 8.81240936e-04, 6.11116819e-04, 1.36836106e-03,\n",
       "         2.74717517e-04, 5.92367316e-04, 1.04860861e-04, 4.22861107e-04,\n",
       "         1.14625174e-04, 2.16092871e-04, 9.62975319e-05, 6.70857044e-05,\n",
       "         1.57746812e-03, 2.68902455e-04, 9.40670259e-04, 1.28601049e-03,\n",
       "         3.80414334e-04, 1.38033251e-03, 4.09328786e-04, 3.91551788e-04,\n",
       "         2.89150979e-04, 9.46893939e-04, 5.85824775e-04, 4.85118944e-04,\n",
       "         6.55861571e-04, 9.94134898e-05, 2.04346306e-03, 8.51190649e-04,\n",
       "         1.48718979e-03, 1.91265426e-03, 9.40920599e-03, 1.10938563e-03,\n",
       "         7.54765875e-04, 5.31783269e-04, 3.65700223e-03, 8.72023695e-04,\n",
       "         4.53626330e-04, 8.60997170e-05, 1.91126252e-04, 5.43009301e-05,\n",
       "         2.71168276e-04, 7.61358897e-05, 1.68635510e-04, 6.74935407e-04,\n",
       "         2.71696714e-04, 5.78861218e-04, 1.42160419e-03, 2.10449914e-03,\n",
       "         3.15737072e-03, 4.59403824e-03, 4.72441141e-04, 2.71064840e-04,\n",
       "         3.83444218e-04, 3.37898353e-04, 5.40875073e-04, 3.36761441e-05,\n",
       "         1.16923300e-04, 4.05741710e-04, 1.60869400e-04, 2.92834942e-04,\n",
       "         4.31972119e-04, 1.36976596e-04, 3.24798282e-04, 1.17935349e-04,\n",
       "         2.69137061e-04, 4.34144691e-04, 9.01693129e-04, 4.07114596e-04,\n",
       "         1.88236256e-04, 4.61661781e-04, 4.49305051e-04, 1.91347106e-04,\n",
       "         3.04531236e-03, 5.79718500e-04, 2.73325969e-03, 4.03131684e-03,\n",
       "         6.75706775e-04, 4.32982604e-04, 1.09325617e-03, 1.32458867e-03,\n",
       "         1.71497901e-04, 1.70520361e-04, 1.32571222e-04, 2.64307688e-04,\n",
       "         1.75707813e-04, 7.39137977e-05, 2.14598913e-04, 4.30997985e-04,\n",
       "         3.57610465e-04, 3.90062865e-04, 1.00081717e-03, 1.35350812e-04,\n",
       "         3.18400416e-04, 6.86836720e-04, 4.13333997e-04, 1.19395430e-04,\n",
       "         5.18052722e-04, 1.07883767e-04, 3.41289124e-04, 3.49052862e-04,\n",
       "         9.70760739e-05, 2.60316563e-04, 4.39063064e-04, 1.66495709e-04,\n",
       "         1.76202258e-04, 3.39403836e-04, 3.69820773e-04, 7.91759230e-04,\n",
       "         7.38917079e-05, 2.81233602e-04, 5.25149633e-04, 9.73012182e-04,\n",
       "         1.60059950e-04, 6.85218489e-04, 1.93108644e-04, 3.90429806e-04,\n",
       "         1.92623884e-05, 1.36164919e-04, 2.31723854e-04, 8.15492531e-05,\n",
       "         7.59855902e-04, 2.18764273e-03, 6.63153260e-05, 1.89940634e-04,\n",
       "         5.60475819e-05, 3.70877853e-04, 7.75837980e-05, 6.93513837e-04,\n",
       "         5.96315134e-04, 4.12382360e-04, 3.00520827e-04, 1.80945208e-04,\n",
       "         4.40961274e-04, 1.19649188e-03, 4.47320798e-03, 1.94465797e-02,\n",
       "         5.45840943e-04, 7.11803732e-04, 2.12936735e-04, 1.79985378e-04,\n",
       "         1.81033407e-04, 8.03568400e-05, 8.62510351e-04, 2.92018376e-04,\n",
       "         1.65944759e-04, 9.21881758e-04, 1.42770325e-04, 6.49523782e-03,\n",
       "         3.53236799e-04, 1.05072965e-03, 5.62944869e-03, 3.52298585e-03,\n",
       "         2.21482624e-04, 3.59319762e-04, 1.26155361e-03, 7.74110085e-05,\n",
       "         3.28680791e-04, 9.22554522e-04, 2.36031367e-04, 4.35306039e-03,\n",
       "         7.70888728e-05, 2.05745193e-04, 4.42038616e-03, 3.36624973e-04,\n",
       "         4.68530372e-04, 6.47130364e-05, 8.42922309e-04, 3.26871697e-04,\n",
       "         1.21638819e-03, 4.73408611e-04, 1.30092143e-04, 2.32112128e-03,\n",
       "         7.76794215e-04, 3.70239810e-04, 8.48395866e-04, 1.54289661e-03,\n",
       "         3.64413572e-04, 6.11729221e-04, 1.44582952e-03, 5.77366853e-04,\n",
       "         4.30339423e-04, 1.15132262e-03, 3.66815075e-04, 3.82844046e-05,\n",
       "         1.18212411e-04, 3.68416047e-04, 7.72218220e-04, 2.55947816e-04,\n",
       "         5.17012995e-05, 1.64887507e-03, 4.96691151e-04, 6.72803901e-04,\n",
       "         3.89837114e-05, 1.20429722e-04, 5.49389608e-03, 2.24540592e-04,\n",
       "         3.64314578e-03, 1.24630646e-03, 2.26157150e-04, 3.70962916e-05,\n",
       "         1.91114217e-04, 1.56220747e-03, 9.53089693e-05, 2.08583195e-03,\n",
       "         4.45793208e-04, 2.32298364e-04, 1.86504665e-04, 2.03166943e-04,\n",
       "         1.54519489e-03, 2.85080809e-04, 3.58292193e-04, 6.76103868e-04,\n",
       "         5.19327703e-04, 1.14240058e-04, 1.97359652e-04, 2.14084936e-03,\n",
       "         4.52673645e-04, 1.31049397e-04, 6.74516778e-04, 4.38539078e-04,\n",
       "         1.11648825e-03, 4.31336317e-04, 2.37900385e-04, 3.99350451e-04,\n",
       "         1.28015480e-03, 4.57555252e-05, 3.74982978e-04, 8.00281050e-05,\n",
       "         3.15006648e-04, 1.61735006e-04, 1.16545016e-04, 5.18734683e-04,\n",
       "         3.43276258e-03, 2.46938958e-04, 2.87841918e-04, 1.47883035e-03,\n",
       "         5.21588186e-03, 5.95951395e-04, 9.55329160e-05, 5.21525391e-04,\n",
       "         2.87019153e-04, 3.48021800e-04, 5.72394521e-04, 1.99648464e-04,\n",
       "         6.76667318e-04, 4.07153554e-03, 5.17055567e-04, 5.32938167e-04,\n",
       "         8.17539927e-04, 5.47891192e-04, 2.98465323e-03, 5.01257484e-04,\n",
       "         3.11660806e-05, 2.47794727e-04, 1.23611372e-03, 7.90322258e-04,\n",
       "         2.49882229e-04, 3.56424251e-04, 1.62606000e-03, 2.57540582e-04,\n",
       "         1.49113257e-04, 4.97765839e-03, 1.76512811e-04, 3.28343704e-05,\n",
       "         1.57930117e-04, 2.20319014e-02, 7.66516241e-05, 2.90518114e-03,\n",
       "         1.76932896e-04, 9.34070908e-04, 1.22528442e-03, 7.21441538e-05,\n",
       "         3.51769477e-03, 3.89123947e-04, 7.80581322e-04, 6.68522262e-04,\n",
       "         3.03353212e-04, 1.23924838e-04, 6.18258273e-05, 1.45413738e-03,\n",
       "         5.40344452e-04, 2.24996737e-04, 1.05031817e-04, 4.52066743e-04,\n",
       "         2.55392515e-04, 9.79326214e-05, 7.85153126e-04, 8.72707969e-05,\n",
       "         7.82493677e-04, 2.10195794e-04, 1.41203718e-03, 6.75280608e-05,\n",
       "         1.19750126e-04, 1.05044735e-03, 2.47032120e-04, 1.76938804e-04,\n",
       "         8.24397794e-05, 3.11388503e-05, 9.36309298e-05, 8.18358967e-04,\n",
       "         5.42789174e-04, 8.69752781e-04, 2.42038412e-04, 8.19887384e-04,\n",
       "         1.60887477e-03, 1.90755096e-03, 3.11481941e-04, 7.10485131e-03,\n",
       "         9.26240231e-04, 5.54266444e-04, 1.11428897e-04, 7.81042836e-05,\n",
       "         8.32207850e-04, 3.89957946e-04, 2.94883095e-04, 4.56492649e-04,\n",
       "         1.53686816e-03, 1.81227177e-03, 1.68436862e-04, 1.21970574e-04,\n",
       "         1.45890494e-03, 2.34183948e-03, 1.41171133e-03, 2.02621188e-04,\n",
       "         1.87275975e-04, 2.36800755e-04, 2.43405625e-03, 5.98049548e-04,\n",
       "         3.73401526e-05, 5.13005129e-04, 1.24862258e-04, 7.95027474e-04,\n",
       "         6.69809640e-04, 6.19520782e-04, 3.58939706e-03, 1.75842305e-03,\n",
       "         2.46155774e-03, 1.84719058e-04, 2.66752415e-03, 2.73783505e-03,\n",
       "         1.22399739e-04, 2.94519850e-04, 1.42714311e-03, 1.21045807e-04,\n",
       "         3.88791232e-04, 1.11517671e-03, 4.64338926e-04, 2.78968085e-03,\n",
       "         5.57180669e-04, 9.25851578e-04, 1.27600506e-04, 5.46568655e-04,\n",
       "         1.37477636e-03, 2.56760832e-04, 2.72622012e-04, 2.05768447e-04,\n",
       "         8.63124791e-04, 5.14221145e-04, 8.56860934e-05, 4.00230987e-04,\n",
       "         1.32682873e-03, 3.34301221e-05, 6.12219796e-04, 2.82619940e-03,\n",
       "         1.13236019e-03, 2.67802505e-04, 7.66904966e-04, 9.76016803e-04,\n",
       "         5.37231914e-04, 1.41205278e-03, 1.68628918e-04, 3.16971767e-04,\n",
       "         3.14541365e-04, 8.60303990e-04, 2.85924849e-04, 2.68171495e-03,\n",
       "         9.67405940e-05, 3.94406416e-05, 3.39998119e-03, 6.95061899e-05,\n",
       "         3.16882593e-04, 1.20228680e-04, 1.46387157e-03, 8.77394559e-05,\n",
       "         5.71425597e-04, 2.02964377e-02, 1.38306830e-04, 1.13313203e-04,\n",
       "         2.30964346e-04, 2.59142183e-03, 2.38983706e-03, 2.02681578e-04,\n",
       "         4.60353855e-04, 1.02399534e-03, 5.64096787e-04, 5.06576442e-04,\n",
       "         4.98159416e-03, 1.43997709e-03, 3.72828130e-04, 3.57034587e-04,\n",
       "         1.14594982e-03, 5.12939121e-04, 1.59480725e-03, 1.16667681e-04,\n",
       "         5.34754188e-04, 2.75671511e-04, 6.80003359e-05, 2.23727594e-03,\n",
       "         2.23605288e-03, 2.98184721e-04, 1.66696365e-04, 2.13748819e-04,\n",
       "         1.02018507e-03, 2.90888624e-04, 7.05696802e-05, 1.92759413e-04,\n",
       "         1.02184629e-02, 1.21236721e-03, 2.02256197e-04, 4.26900988e-05,\n",
       "         6.88777887e-04, 4.27623636e-05, 1.62205659e-04, 2.87755189e-04,\n",
       "         6.95374154e-04, 1.89137156e-03, 6.53928262e-04, 1.25423749e-03,\n",
       "         1.04038017e-02, 5.56827290e-03, 2.03694706e-03, 6.33171643e-04,\n",
       "         3.09530500e-04, 3.38896971e-05, 3.45342618e-04, 1.14330847e-03,\n",
       "         2.46436568e-03, 2.19109003e-03, 3.33191850e-03, 3.92786926e-04,\n",
       "         1.01073776e-04, 1.29900791e-03, 4.34008194e-04, 4.56512120e-04,\n",
       "         6.26954855e-03, 9.43053223e-04, 2.08644677e-04, 2.12482479e-03,\n",
       "         1.90837382e-04, 2.26989898e-04, 1.01906880e-04, 4.53725515e-04,\n",
       "         9.05461420e-05, 4.94131760e-04, 2.85371847e-04, 1.27227057e-03,\n",
       "         3.72119102e-04, 4.19156830e-04, 8.95913574e-04, 2.12762156e-04,\n",
       "         5.58666943e-04, 1.40568335e-03, 1.37875334e-03, 3.92678514e-04,\n",
       "         1.87386468e-03, 2.03020242e-03, 3.47752287e-03, 3.89993656e-04,\n",
       "         1.04353239e-03, 2.60099326e-03, 2.18038549e-04, 9.72013979e-04,\n",
       "         3.62138322e-04, 1.52921348e-04, 3.51933151e-04, 1.43848374e-04,\n",
       "         2.33719987e-03, 3.20235477e-03, 4.80017552e-05, 6.66620792e-04,\n",
       "         9.33128933e-04, 4.95977758e-04, 1.07232663e-04, 3.72631592e-03,\n",
       "         5.45184128e-04, 2.89862929e-03, 5.08502882e-04, 7.67151592e-04,\n",
       "         2.53705843e-03, 1.25303050e-03, 9.37042700e-04, 1.31417546e-04,\n",
       "         1.53174435e-04, 6.29629300e-04, 1.70811219e-03, 6.77616263e-05,\n",
       "         4.28867148e-04, 1.12778791e-04, 5.27410069e-04, 1.95997139e-03,\n",
       "         2.08363310e-03, 6.25769026e-04, 4.72745596e-04, 5.59936394e-04,\n",
       "         1.22532598e-04, 1.80609280e-03, 1.71314983e-04, 4.09603032e-04,\n",
       "         1.14634121e-03, 3.68677196e-03, 1.12885628e-02, 3.65139771e-04,\n",
       "         2.95391714e-04, 6.99059747e-04, 1.11751736e-03, 1.02187996e-03,\n",
       "         4.03747945e-05, 5.02108480e-04, 1.80625910e-04, 5.64439280e-04,\n",
       "         2.65366444e-03, 5.20578003e-04, 2.32088234e-04, 2.28015371e-04,\n",
       "         2.24274685e-04, 9.33560019e-04, 7.86489341e-04, 7.59713934e-04,\n",
       "         1.73447840e-03, 3.59636708e-03, 1.52002569e-04, 7.09315704e-04,\n",
       "         1.39476219e-03, 5.05656644e-05, 7.95369153e-04, 3.88760418e-05,\n",
       "         7.77309542e-05, 1.20087963e-04, 9.67054075e-05, 7.61443167e-04,\n",
       "         2.80150940e-04, 1.49003943e-04, 5.21549897e-04, 1.27074440e-04,\n",
       "         1.89778581e-03, 4.16680123e-05, 3.42723593e-04, 1.51928794e-03,\n",
       "         3.60803184e-04, 2.65615585e-04, 2.69393582e-04, 1.89385144e-04,\n",
       "         5.61156834e-04, 6.65584987e-04, 1.92560174e-03, 1.26803279e-04,\n",
       "         1.24207325e-03, 3.42029438e-04, 6.11481199e-04, 1.63564560e-04,\n",
       "         2.37319944e-03, 4.97092120e-03, 1.04777818e-03, 1.41248704e-04,\n",
       "         4.99900023e-04, 1.47110422e-03, 1.09786028e-03, 7.54895853e-04,\n",
       "         6.18807506e-04, 8.77315033e-05, 4.98867485e-05, 1.67192589e-03,\n",
       "         3.85956409e-05, 1.26400555e-04, 2.78276886e-04, 1.29942049e-03,\n",
       "         6.07848488e-05, 8.07544123e-03, 2.52558442e-04, 9.68164240e-05,\n",
       "         6.62381790e-05, 3.71968672e-05, 8.65825641e-05, 1.37795607e-04,\n",
       "         4.30142367e-03, 1.92502048e-04, 2.09807942e-04, 1.61321383e-04,\n",
       "         4.34279704e-04, 1.39671291e-04, 6.57804994e-05, 3.09913012e-04,\n",
       "         2.57831789e-03, 2.24945572e-04, 1.03258535e-04, 1.81269133e-04,\n",
       "         2.32219740e-04, 1.50739870e-04, 7.69620878e-04, 1.59682590e-03,\n",
       "         3.60700930e-03, 7.85678742e-04, 9.36860451e-05, 2.01791408e-04,\n",
       "         1.29630818e-04, 1.52543944e-04, 3.28055292e-04, 4.58392198e-04,\n",
       "         4.42062970e-03, 1.85260980e-03, 1.01297360e-03, 8.42403562e-04,\n",
       "         1.93191261e-03, 3.01046413e-03, 1.93770835e-03, 1.83438987e-03,\n",
       "         8.23461975e-04, 2.92547559e-03, 1.34752435e-03, 2.34879437e-04,\n",
       "         1.37894170e-03, 9.40489525e-04, 5.81231841e-04, 5.16647822e-04,\n",
       "         3.35135427e-03, 5.53897582e-04, 1.92740129e-03, 1.54179148e-03,\n",
       "         8.49086719e-05, 6.03248889e-04, 3.64696753e-04, 2.81035318e-04,\n",
       "         8.76193371e-05, 8.60328219e-05, 1.41460728e-03, 5.66225033e-04,\n",
       "         1.84378412e-04, 4.87939455e-03, 2.49372050e-03, 3.86616593e-04,\n",
       "         8.96303463e-05, 1.16213341e-03, 6.97021387e-05, 2.77492974e-04,\n",
       "         5.15022548e-04, 9.15360346e-04, 2.80282402e-04, 8.41373869e-04,\n",
       "         3.53670446e-04, 3.23198037e-04, 2.46025622e-04, 5.70198346e-04,\n",
       "         2.01129427e-04, 1.84116245e-04, 4.43208846e-04, 1.18980090e-04,\n",
       "         3.01398366e-04, 1.64530778e-04, 1.09163579e-03, 2.50629586e-04,\n",
       "         1.54657406e-04, 1.52803477e-04, 6.85713312e-05, 2.16370347e-04,\n",
       "         7.59394490e-04, 8.24551535e-05, 1.89842103e-04, 2.79237487e-04,\n",
       "         2.32581893e-04, 1.22776590e-04, 3.99267999e-04, 8.54006794e-05,\n",
       "         3.14791774e-04, 1.69303923e-04, 1.59074683e-04, 1.95737157e-04,\n",
       "         1.89513245e-04, 1.43063059e-02, 8.70992590e-05, 9.10963645e-05,\n",
       "         2.30187798e-04, 1.36793184e-03, 5.05920267e-04, 5.28384524e-04,\n",
       "         6.18646096e-04, 5.09394507e-04, 4.03863465e-04, 1.38606236e-03,\n",
       "         1.87698024e-04, 4.32883462e-05, 5.08297002e-04, 1.99245522e-04,\n",
       "         1.86248799e-04, 3.19615519e-03, 2.45784788e-04, 1.29642372e-04,\n",
       "         4.86094475e-04, 2.07353733e-04, 3.50585149e-04, 1.17391741e-04,\n",
       "         1.35770068e-04, 1.68759958e-04, 4.82100368e-05, 2.98058323e-04,\n",
       "         4.19122167e-04, 1.58440089e-04, 3.82830367e-05, 6.46449989e-05,\n",
       "         1.46329592e-04, 2.41266007e-05, 1.23517297e-04, 1.96381952e-04,\n",
       "         1.47064493e-04, 1.24978891e-04, 1.76649381e-04, 2.05068979e-02]],\n",
       "       dtype=float32)>]"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This comes in handy for tasks like\n",
    "[neural style transfer](https://keras.io/examples/generative/neural_style_transfer/),\n",
    "among other things."
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extend the API using custom layers\n",
    "\n",
    "`tf.keras` includes a wide range of built-in layers, for example:\n",
    "\n",
    "- Convolutional layers: `Conv1D`, `Conv2D`, `Conv3D`, `Conv2DTranspose`\n",
    "- Pooling layers: `MaxPooling1D`, `MaxPooling2D`, `MaxPooling3D`, `AveragePooling1D`\n",
    "- RNN layers: `GRU`, `LSTM`, `ConvLSTM2D`\n",
    "- `BatchNormalization`, `Dropout`, `Embedding`, etc.\n",
    "\n",
    "But if you don't find what you need, it's easy to extend the API by creating\n",
    "your own layers. All layers subclass the `Layer` class and implement:\n",
    "\n",
    "- `call` method, that specifies the computation done by the layer.\n",
    "- `build` method, that creates the weights of the layer (this is just a style\n",
    "convention since you can create weights in `__init__`, as well).\n",
    "\n",
    "To learn more about creating layers from scratch, read\n",
    "[custom layers and models](/guides/making_new_layers_and_models_via_subclassing) guide.\n",
    "\n",
    "The following is a basic implementation of `tf.keras.layers.Dense`:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "class CustomDense(layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(CustomDense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "inputs = keras.Input((4,))\n",
    "outputs = CustomDense(10)(inputs)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For serialization support in your custom layer, define a `get_config`\n",
    "method that returns the constructor arguments of the layer instance:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "class CustomDense(layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(CustomDense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"units\": self.units}\n",
    "\n",
    "\n",
    "inputs = keras.Input((4,))\n",
    "outputs = CustomDense(10)(inputs)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "config = model.get_config()\n",
    "\n",
    "new_model = keras.Model.from_config(config, custom_objects={\"CustomDense\": CustomDense})"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optionally, implement the class method `from_config(cls, config)` which is used\n",
    "when recreating a layer instance given its config dictionary.\n",
    "The default implementation of `from_config` is:\n",
    "\n",
    "```python\n",
    "def from_config(cls, config):\n",
    "  return cls(**config)\n",
    "```"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## When to use the functional API\n",
    "\n",
    "Should you use the Keras functional API to create a new model,\n",
    "or just subclass the `Model` class directly? In general, the functional API\n",
    "is higher-level, easier and safer, and has a number of\n",
    "features that subclassed models do not support.\n",
    "\n",
    "However, model subclassing provides greater flexibility when building models\n",
    "that are not easily expressible as directed acyclic graphs of layers.\n",
    "For example, you could not implement a Tree-RNN with the functional API\n",
    "and would have to subclass `Model` directly.\n",
    "\n",
    "For an in-depth look at the differences between the functional API and\n",
    "model subclassing, read\n",
    "[What are Symbolic and Imperative APIs in TensorFlow 2.0?](https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html).\n",
    "\n",
    "### Functional API strengths:\n",
    "\n",
    "The following properties are also true for Sequential models\n",
    "(which are also data structures), but are not true for subclassed models\n",
    "(which are Python bytecode, not data structures).\n",
    "\n",
    "#### Less verbose\n",
    "\n",
    "There is no `super(MyClass, self).__init__(...)`, no `def call(self, ...):`, etc.\n",
    "\n",
    "Compare:\n",
    "\n",
    "```python\n",
    "inputs = keras.Input(shape=(32,))\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "outputs = layers.Dense(10)(x)\n",
    "mlp = keras.Model(inputs, outputs)\n",
    "```\n",
    "\n",
    "With the subclassed version:\n",
    "\n",
    "```python\n",
    "class MLP(keras.Model):\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super(MLP, self).__init__(**kwargs)\n",
    "    self.dense_1 = layers.Dense(64, activation='relu')\n",
    "    self.dense_2 = layers.Dense(10)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.dense_1(inputs)\n",
    "    return self.dense_2(x)\n",
    "\n",
    "# Instantiate the model.\n",
    "mlp = MLP()\n",
    "# Necessary to create the model's state.\n",
    "# The model doesn't have a state until it's called at least once.\n",
    "_ = mlp(tf.zeros((1, 32)))\n",
    "```\n",
    "\n",
    "#### Model validation while defining its connectivity graph\n",
    "\n",
    "In the functional API, the input specification (shape and dtype) is created\n",
    "in advance (using `Input`). Every time you call a layer,\n",
    "the layer checks that the specification passed to it matches its assumptions,\n",
    "and it will raise a helpful error message if not.\n",
    "\n",
    "This guarantees that any model you can build with the functional API will run.\n",
    "All debugging -- other than convergence-related debugging --\n",
    "happens statically during the model construction and not at execution time.\n",
    "This is similar to type checking in a compiler.\n",
    "\n",
    "#### A functional model is plottable and inspectable\n",
    "\n",
    "You can plot the model as a graph, and you can easily access intermediate nodes\n",
    "in this graph. For example, to extract and reuse the activations of intermediate\n",
    "layers (as seen in a previous example):\n",
    "\n",
    "```python\n",
    "features_list = [layer.output for layer in vgg19.layers]\n",
    "feat_extraction_model = keras.Model(inputs=vgg19.input, outputs=features_list)\n",
    "```\n",
    "\n",
    "#### A functional model can be serialized or cloned\n",
    "\n",
    "Because a functional model is a data structure rather than a piece of code,\n",
    "it is safely serializable and can be saved as a single file\n",
    "that allows you to recreate the exact same model\n",
    "without having access to any of the original code.\n",
    "See the [serialization & saving guide](/guides/serialization_and_saving/).\n",
    "\n",
    "To serialize a subclassed model, it is necessary for the implementer\n",
    "to specify a `get_config()`\n",
    "and `from_config()` method at the model level.\n",
    "\n",
    "\n",
    "### Functional API weakness:\n",
    "\n",
    "#### It does not support dynamic architectures\n",
    "\n",
    "The functional API treats models as DAGs of layers.\n",
    "This is true for most deep learning architectures, but not all -- for example,\n",
    "recursive networks or Tree RNNs do not follow this assumption and cannot\n",
    "be implemented in the functional API."
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mix-and-match API styles\n",
    "\n",
    "Choosing between the functional API or Model subclassing isn't a\n",
    "binary dec## Mix-and-match API styles\n",
    "\n",
    "Choosing between the functional API or Model subclassing isn't a\n",
    "binary decision that restricts you into one category of models.\n",
    "All models in the `tf.keras` API can interact with each other, whether they're\n",
    "`Sequential` models, functional models, or subclassed models that are written\n",
    "from scratch.\n",
    "\n",
    "You can always use a functional model or `Sequential` model\n",
    "as part of a subclassed model or layer:ision that restricts you into one category of models.\n",
    "All models in the `tf.keras` API can interact with each other, whether they're\n",
    "`Sequential` models, functional models, or subclassed models that are written\n",
    "from scratch.\n",
    "\n",
    "You can always use a functional model or `Sequential` model\n",
    "as part of a subclassed model or layer:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "units = 32\n",
    "timesteps = 10\n",
    "input_dim = 5\n",
    "\n",
    "# Define a Functional model\n",
    "inputs = keras.Input((None, units))\n",
    "x = layers.GlobalAveragePooling1D()(inputs)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "class CustomRNN(layers.Layer):\n",
    "    #def __init__(self):\n",
    "    def __init__(self, my_units, my_model):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.units = my_units\n",
    "        self.projection_1 = layers.Dense(units=units, activation=\"tanh\")\n",
    "        self.projection_2 = layers.Dense(units=units, activation=\"tanh\")\n",
    "        # Our previously-defined Functional model\n",
    "        self.classifier = my_model- `call(sel- `call(self, inputs, **kwargs)` -- f, inputs, **kwargs)` -- \n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = []\n",
    "        state = tf.zeros(shape=(inputs.shape[0], self.units))\n",
    "        for t in range(inputs.shape[1]):\n",
    "            x = inputs[:, t, :]\n",
    "            h = self.projection_1(x)\n",
    "            y = h + self.projection_2(state)\n",
    "            state = y\n",
    "            outputs.append(y)\n",
    "        features = tf.stack(outputs, axis=1)\n",
    "        print(f\"feature shape {features.shape}\")\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "#rnn_model = CustomRNN()\n",
    "rnn_model = CustomRNN(units, model)\n",
    "_ = rnn_model(tf.zeros((1, timesteps, input_dim)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "feature shape (1, 10, 32)\n"
     ]
    }
   ],
   "metadata": {
    "colab_type": "code"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use any subclassed layer or model in the functional API\n",
    "as long as it implements a `call` method that follows one of the following patterns:\n",
    "\n",
    "- `call(self, inputs, **kwargs)` --\n",
    "Where `inputs` is a tensor or a nested structure of tensors (e.g. a list of tensors),\n",
    "and where `**kwargs` are non-tensor arguments (non-inputs).\n",
    "- `call(self, inputs, training=None, **kwargs)` --\n",
    "Where `training` is a boolean indicating whether the layer should behave\n",
    "in training mode and inference mode.\n",
    "- `call(self, inputs, mask=None, **kwargs)` --\n",
    "Where `mask` is a boolean mask tensor (useful for RNNs, for instance).\n",
    "- `call(self, inputs, training=None, mask=None, **kwargs)` --\n",
    "Of course, you can have both masking and training-specific behavior at the same time.\n",
    "\n",
    "Additionally, if you implement the `get_config` method on your custom Layer or model,\n",
    "the functional models you create will still be serializable and cloneable.\n",
    "\n",
    "Here's a quick example of a custom RNN, written from scratch,\n",
    "being used in a functional model:"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "units = 32\n",
    "timesteps = 10\n",
    "input_dim = 5\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "class CustomRNN(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.units = units\n",
    "        self.projection_1 = layers.Dense(units=units, activation=\"tanh\")\n",
    "        self.projection_2 = layers.Dense(units=units, activation=\"tanh\")\n",
    "        self.classifier = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = []\n",
    "        state = tf.zeros(shape=(inputs.shape[0], self.units))\n",
    "        for t in range(inputs.shape[1]):\n",
    "            x = inputs[:, t, :]\n",
    "            h = self.projection_1(x)\n",
    "            y = h + self.projection_2(state)\n",
    "            state = y\n",
    "            outputs.append(y)\n",
    "        features = tf.stack(outputs, axis=1)\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "# Note that you specify a static batch size for the inputs with the `batch_shape`\n",
    "# arg, because the inner computation of `CustomRNN` requires a static batch size\n",
    "# (when you create the `state` zeros tensor).\n",
    "inputs = keras.Input(batch_shape=(batch_size, timesteps, input_dim))\n",
    "x = layers.Conv1D(32, 3)(inputs)\n",
    "outputs = CustomRNN()(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "rnn_model = CustomRNN()\n",
    "_ = rnn_model(tf.zeros((1, 10, 5)))"
   ],
   "outputs": [],
   "metadata": {
    "colab_type": "code"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "functional_api",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('kerasio': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "7a252f18ce85ad2ddc455c547d26eed4edf562814fbc67e6eff7efc68fad314c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}